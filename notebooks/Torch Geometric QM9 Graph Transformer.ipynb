{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff921ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets.qm9 import QM9\n",
    "import torch_geometric.datasets.qm9 as qm9\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch_geometric.nn as tgnn\n",
    "from torch_scatter import scatter\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8b07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] Reports MAE in eV / Chemical Accuracy of the target variable U0. \n",
    "# The chemical accuracy of U0 is 0.043 see [1, Table 5].\n",
    "\n",
    "# Reproduced table [0]\n",
    "# MXMNet: 0.00590/0.043 = 0.13720930232558143\n",
    "# HMGNN:  0.00592/0.043 = 0.13767441860465118\n",
    "# MPNN:   0.01935/0.043 = 0.45\n",
    "# KRR:    0.0251 /0.043 = 0.5837209302325582\n",
    "# [0] https://paperswithcode.com/sota/formation-energy-on-qm9\n",
    "# [1] Neural Message Passing for Quantum Chemistry, https://arxiv.org/pdf/1704.01212v2.pdf\n",
    "# MXMNet https://arxiv.org/pdf/2011.07457v1.pdf\n",
    "# HMGNN https://arxiv.org/pdf/2009.12710v1.pdf\n",
    "# MPNN https://arxiv.org/pdf/1704.01212v2.pdf\n",
    "# KRR HDAD kernel ridge regression https://arxiv.org/pdf/1702.05532.pdf\n",
    "# HDAD means HDAD (Histogram of distances, anglesand dihedral angles)\n",
    "\n",
    "# [2] Reports the average value of MAE / Chemical Accuracy of over all targets\n",
    "# [2] https://paperswithcode.com/sota/drug-discovery-on-qm9\n",
    "target_dict = {0: 'mu, D, Dipole moment', \n",
    "               1: 'alpha, {a_0}^3, Isotropic polarizability', \n",
    "               2: 'epsilon_{HOMO}, eV, Highest occupied molecular orbital energy',\n",
    "               3: 'epsilon_{LUMO}, eV, Lowest unoccupied molecular orbital energy',\n",
    "               4: 'Delta, eV, Gap between HOMO and LUMO',\n",
    "               5: '< R^2 >, {a_0}^2, Electronic spatial extent',\n",
    "               6: 'ZPVE, eV, Zero point vibrational energy', \n",
    "               7: 'U_0, eV, Internal energy at 0K',\n",
    "               8: 'U, eV, Internal energy at 298.15K', \n",
    "               9: 'H, eV, Enthalpy at 298.15K',\n",
    "               10: 'G, eV, Free energy at 298.15K',  \n",
    "               11: 'c_{v}, cal\\(mol K), Heat capacity at 298.15K'}\n",
    "\n",
    "chemical_accuracy = {idx:0.043 for idx in range(12)}\n",
    "chemical_accuracy[0] = 0.1\n",
    "chemical_accuracy[1] = 0.1\n",
    "chemical_accuracy[5] = 1.2\n",
    "chemical_accuracy[6] = 0.0012\n",
    "chemical_accuracy[11] = 0.050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c1b00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mnts45o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 14708<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\Chris Wendler\\repos\\2021\\graph-transformer\\notebooks\\wandb\\run-20210715_153153-1mnts45o\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\Chris Wendler\\repos\\2021\\graph-transformer\\notebooks\\wandb\\run-20210715_153153-1mnts45o\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>MSE</td><td>0.10969</td></tr><tr><td>_runtime</td><td>17931</td></tr><tr><td>_timestamp</td><td>1626373849</td></tr><tr><td>_step</td><td>218911</td></tr><tr><td>MAE</td><td>0.24247</td></tr><tr><td>MAE/CA</td><td>5.63893</td></tr><tr><td>learning rate</td><td>0.00018</td></tr><tr><td>TEST MAE</td><td>0.3035</td></tr><tr><td>TEST MAE/CA</td><td>7.05805</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>MSE</td><td>▆▆██▅▅▇▅▅▄▅▆▄▄▄▄▃▄▆▃▆▄▃▂▂▃▂▂▂▂▁▂▂▂▁▁▂▁▂▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>MAE</td><td>█▇█▆▆▅▅▅▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>MAE/CA</td><td>█▇█▆▆▅▅▅▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>learning rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>TEST MAE</td><td>▁</td></tr><tr><td>TEST MAE/CA</td><td>▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">skilled-voice-6</strong>: <a href=\"https://wandb.ai/chrisxx/QM9-transformer/runs/1mnts45o\" target=\"_blank\">https://wandb.ai/chrisxx/QM9-transformer/runs/1mnts45o</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1mnts45o). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">warm-surf-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chrisxx/QM9-transformer\" target=\"_blank\">https://wandb.ai/chrisxx/QM9-transformer</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chrisxx/QM9-transformer/runs/3k911a7f\" target=\"_blank\">https://wandb.ai/chrisxx/QM9-transformer/runs/3k911a7f</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Chris Wendler\\repos\\2021\\graph-transformer\\notebooks\\wandb\\run-20210715_203300-3k911a7f</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='QM9-transformer', entity='chrisxx')\n",
    "config = wandb.config\n",
    "config.lr = 0.0003\n",
    "config.n_epochs = 2000\n",
    "config.patience = 5\n",
    "config.factor = 0.95\n",
    "config.minimal_lr = 6e-8\n",
    "config.target_idx = 7\n",
    "config.batch_size = 128\n",
    "config.n_train = 110000\n",
    "config.n_valid = 10000\n",
    "config.target_ratio = 0.4\n",
    "config.store_starting_from_ratio = 1\n",
    "config.required_improvement = 0.8\n",
    "config.model_dir = '../models/qm9/gtransformer_big/'\n",
    "config.dfs_codes = '../datasets/qm9_geometric/min_dfs_codes.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2606045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = config.target_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47edd3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e179c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM9DFSCodes(Dataset):\n",
    "    def __init__(self, dfs_codes, qm9_dataset, \n",
    "                 target_idx, vert_feats = ['x', 'pos'],\n",
    "                 vertex_transform=None, edge_transform=None):\n",
    "        self.dfs_codes = dfs_codes\n",
    "        self.qm9_dataset = qm9_dataset\n",
    "        self.target_idx = target_idx\n",
    "        self.vertex_features = vert_feats\n",
    "        self.max_vert = 29\n",
    "        self.max_len = np.max([len(d['min_dfs_code']) for d in self.dfs_codes.values()])\n",
    "        self.feat_dim = 2 + qm9_dataset[0].edge_attr.shape[1] # 2 for the dfs indices\n",
    "        for feat in vert_feats: \n",
    "            self.feat_dim += 2*qm9_dataset[0][feat].shape[1]\n",
    "        self.vertex_transform = vertex_transform\n",
    "        self.edge_transform = edge_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qm9_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        graph_repr: [batch_size, max_edges, 2 + n_vert_feat + n_edge_feat + n_vert_feat]\n",
    "        return vertex_features, graph_repr, target\n",
    "        \"\"\"\n",
    "        data = self.qm9_dataset[idx]\n",
    "        code_dict = self.dfs_codes[data.name]\n",
    "        code = code_dict['min_dfs_code']\n",
    "        eid2nr = code_dict['edge_id_2_edge_number']\n",
    "        \n",
    "        vert_feats = [data[k].detach().cpu().numpy() for k in ['x', 'pos']]\n",
    "        vert_feats = np.concatenate(vert_feats, axis=1)\n",
    "        edge_feats = data.edge_attr.detach().cpu().numpy()\n",
    "        \n",
    "        d = {'dfs_from':np.zeros(self.max_len), \n",
    "             'dfs_to':np.zeros(self.max_len),\n",
    "             'feat_from':np.zeros((self.max_len, vert_feats.shape[1])), \n",
    "             'feat_to':np.zeros((self.max_len, vert_feats.shape[1])), \n",
    "             'feat_edge':np.zeros((self.max_len, edge_feats.shape[1])),\n",
    "             'n_edges':len(code)*np.ones((1,), dtype=np.int32),\n",
    "             'z':np.zeros(self.max_vert)}\n",
    "        \n",
    "        for idx, e_tuple in enumerate(code):\n",
    "            e_idx = eid2nr[str(e_tuple[-2])]\n",
    "            from_idx = e_tuple[-3]\n",
    "            to_idx = e_tuple[-1]\n",
    "            d['dfs_from'][idx] = e_tuple[0]\n",
    "            d['dfs_to'][idx] = e_tuple[1]\n",
    "            d['feat_from'][idx] = vert_feats[from_idx]\n",
    "            d['feat_to'][idx] = vert_feats[to_idx]\n",
    "            d['feat_edge'][idx] = edge_feats[e_idx]      \n",
    "        \n",
    "        d['z'][:len(data.z)] = data.z.detach().cpu().numpy()\n",
    "        \n",
    "        d_tensors = {}\n",
    "        d_tensors['dfs_from'] = torch.IntTensor(d['dfs_from'])\n",
    "        d_tensors['dfs_to'] = torch.IntTensor(d['dfs_to'])\n",
    "        d_tensors['feat_from'] = torch.Tensor(d['feat_from'])\n",
    "        d_tensors['feat_to'] = torch.Tensor(d['feat_to'])\n",
    "        d_tensors['feat_edge'] = torch.Tensor(d['feat_edge'])\n",
    "        d_tensors['z'] = torch.IntTensor(d['z'])\n",
    "        \n",
    "        if self.vertex_transform:\n",
    "            d_tensors['feat_from'] = self.vertex_transform(d_tensors['feat_from'])\n",
    "            d_tensors['feat_to'] = self.vertex_transform(d_tensors['feat_to'])\n",
    "        if self.edge_transform:\n",
    "            d_tensors['feat_edge'] = self.edge_transform(d_tensors['feat_edge'])\n",
    "        \n",
    "        d_tensors['target'] = data.y[:, self.target_idx]\n",
    "        return d_tensors\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.qm9_dataset = self.qm9_dataset.shuffle()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3616777",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.dfs_codes, 'r') as f:\n",
    "    dfs_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = QM9('../datasets/qm9_geometric/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd471b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dset.shuffle()\n",
    "train_qm9 = DataLoader(dset[:config.n_train], batch_size=config.batch_size)\n",
    "train_dataset = QM9DFSCodes(dfs_codes, dset[:config.n_train], target_idx)\n",
    "valid_dataset = QM9DFSCodes(dfs_codes, dset[config.n_train:config.n_train+config.n_valid], target_idx) \n",
    "test_dataset = QM9DFSCodes(dfs_codes, dset[config.n_train+config.n_valid:], target_idx) \n",
    "config.n_test = len(test_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a94cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb6baac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:05:52.857406, resuming normal operation.\n"
     ]
    }
   ],
   "source": [
    "torch.save(dset.indices(), config.model_dir+'dataset_indices.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be0c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12914486",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64f544b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeTransformer(nn.Module):\n",
    "    def __init__(self, vert_dim, edge_dim, d_model=512, nhead=8, nlayers=4, dim_feedforward=2048, mean=None, std=None, atomref=None,\n",
    "                 max_vertices=29, max_edges=28):\n",
    "        \"\"\"\n",
    "        transfomer model is some type of transformer that \n",
    "        \"\"\"\n",
    "        super(MoleculeTransformer, self).__init__()\n",
    "        # atomic masses could be used as additional features\n",
    "        # see https://github.com/rusty1s/pytorch_geometric/blob/97d3177dc43858f66c07bb66d7dc12506b986199/torch_geometric/nn/models/schnet.py#L113\n",
    "        self.vert_dim = vert_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.nlayers = nlayers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.max_vertices = max_vertices\n",
    "        self.max_edges = max_edges\n",
    "        \n",
    "        self.emb_dfs = nn.Embedding(self.max_vertices, d_model // 2)\n",
    "        self.emb_vertex = nn.Linear(self.vert_dim, d_model // 2)\n",
    "        self.emb_edge = nn.Linear(self.edge_dim, d_model)        \n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, self.d_model), requires_grad=True)\n",
    "        self.enc = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead, dim_feedforward=self.dim_feedforward), self.nlayers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(self.d_model, 1)\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.register_buffer('initial_atomref', atomref)\n",
    "        self.atomref = None\n",
    "        if atomref is not None:\n",
    "            self.atomref = nn.Embedding(100, 1)\n",
    "            self.atomref.weight.data.copy_(atomref)\n",
    "            \n",
    "        nn.init.normal_(self.cls_token, mean=.0, std=.5)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        z = data['z']\n",
    "        dfs_from_emb = self.emb_dfs(data['dfs_from'])\n",
    "        dfs_to_emb = self.emb_dfs(data['dfs_to'])\n",
    "        dfs_emb = torch.cat((dfs_from_emb, dfs_to_emb), -1)\n",
    "        from_emb = self.emb_vertex(data['feat_from'])\n",
    "        to_emb = self.emb_vertex(data['feat_to'])\n",
    "        feat_emb = torch.cat((from_emb, to_emb), -1)\n",
    "        edge_emb = self.emb_edge(data['feat_edge'])\n",
    "        batch = dfs_emb + feat_emb + edge_emb # batch_dim x seq_dim x n_model\n",
    "        batch = batch.permute(1, 0, 2) # seq_dim x batch_dim x n_model\n",
    "        batch = torch.cat((self.cls_token.expand(-1, batch.shape[1], -1), batch), dim=0)\n",
    "        \n",
    "        transformer_out = self.enc(batch)\n",
    "        out = self.fc_out(transformer_out[0]) \n",
    "        \n",
    "        # tricks from Schnet\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            out = out * self.std + self.mean\n",
    "        \n",
    "        if self.atomref is not None:\n",
    "            out = out + torch.sum(self.atomref(z), axis=1)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d7c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786e4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://schnetpack.readthedocs.io/en/stable/tutorials/tutorial_02_qm9.html\n",
    "# and https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/schnet.html#SchNet\n",
    "for data in train_qm9:\n",
    "    data = data.to(device)\n",
    "    atomU0s = torch.tensor(qm9.atomrefs[target_idx], device=device)[torch.argmax(data.x[:, :5], axis=1)]\n",
    "    target_modular = scatter(atomU0s, data.batch, dim=-1, reduce='sum')\n",
    "    target_vec += [(data.y[:, target_idx] - target_modular).detach().cpu().numpy()]\n",
    "target_vec = np.concatenate(target_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c96e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = np.mean(target_vec)\n",
    "target_std = np.std(target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a80a7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(train_loader))\n",
    "vert_dim = d['feat_from'].shape[-1]\n",
    "edge_dim = d['feat_edge'].shape[-1]\n",
    "model = MoleculeTransformer(vert_dim, edge_dim, atomref=dset.atomref(target_idx), mean=target_mean, std=target_std)\n",
    "\n",
    "loss = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', verbose=True, patience=config.patience, factor=config.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28549be9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66509662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e4a46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: MAE/CA 15.805389: : 860it [01:02, 13.74it/s]\n",
      "Epoch 2: MAE/CA 15.169381: : 860it [01:05, 13.21it/s]\n",
      "Epoch 3: MAE/CA 14.145570: : 860it [01:04, 13.40it/s]\n",
      "Epoch 4: MAE/CA 14.015864: : 860it [01:00, 14.30it/s]\n",
      "Epoch 5: MAE/CA 14.914134: : 860it [01:03, 13.61it/s]\n",
      "Epoch 6: MAE/CA 13.536842: : 860it [01:05, 13.18it/s]\n",
      "Epoch 7: MAE/CA 13.866200: : 860it [01:01, 14.02it/s]\n",
      "Epoch 8: MAE/CA 13.278941: : 860it [00:59, 14.48it/s]\n",
      "Epoch 9: MAE/CA 14.147046: : 860it [01:03, 13.65it/s]\n",
      "Epoch 10: MAE/CA 13.836421: : 860it [01:03, 13.60it/s]\n",
      "Epoch 11: MAE/CA 13.987762: : 860it [01:09, 12.40it/s]\n",
      "Epoch 12: MAE/CA 12.753078: : 860it [01:12, 11.88it/s]\n",
      "Epoch 13: MAE/CA 12.444085: : 860it [01:03, 13.63it/s]\n",
      "Epoch 14: MAE/CA 14.526193: : 860it [01:00, 14.22it/s]\n",
      "Epoch 15: MAE/CA 16.034590: : 860it [01:06, 12.90it/s]\n",
      "Epoch 16: MAE/CA 13.333688: : 860it [01:07, 12.77it/s]\n",
      "Epoch 17: MAE/CA 12.821490: : 860it [01:07, 12.71it/s]\n",
      "Epoch 18: MAE/CA 13.096951: : 860it [01:07, 12.74it/s]\n",
      "Epoch 19: MAE/CA 13.729136: : 860it [01:06, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    29: reducing learning rate of group 0 to 2.8500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: MAE/CA 12.591199: : 860it [01:06, 13.03it/s]\n",
      "Epoch 21: MAE/CA 12.906681: : 860it [01:07, 12.67it/s]\n",
      "Epoch 22: MAE/CA 13.126674: : 860it [01:06, 12.91it/s]\n",
      "Epoch 23: MAE/CA 12.135311: : 860it [01:06, 12.99it/s]\n",
      "Epoch 24: MAE/CA 13.478188: : 860it [01:06, 12.93it/s]\n",
      "Epoch 25: MAE/CA 13.308612: : 860it [01:06, 12.94it/s]\n",
      "Epoch 26: MAE/CA 12.776156: : 860it [01:06, 12.91it/s]\n",
      "Epoch 27: MAE/CA 12.942582: : 860it [01:06, 12.92it/s]\n",
      "Epoch 28: MAE/CA 12.129993: : 860it [01:06, 12.94it/s]\n",
      "Epoch 29: MAE/CA 12.159268: : 860it [01:06, 12.95it/s]\n",
      "Epoch 30: MAE/CA 12.166283: : 860it [01:06, 12.97it/s]\n",
      "Epoch 31: MAE/CA 12.184331: : 860it [01:06, 12.99it/s]\n",
      "Epoch 32: MAE/CA 12.573886: : 860it [01:06, 12.92it/s]\n",
      "Epoch 33: MAE/CA 12.591450: : 860it [01:06, 12.87it/s]\n",
      "Epoch 34: MAE/CA 11.577575: : 860it [01:06, 12.85it/s]\n",
      "Epoch 35: MAE/CA 11.775059: : 860it [01:07, 12.81it/s]\n",
      "Epoch 36: MAE/CA 11.858796: : 860it [01:06, 12.84it/s]\n",
      "Epoch 37: MAE/CA 12.562565: : 860it [01:06, 12.89it/s]\n",
      "Epoch 38: MAE/CA 11.722942: : 860it [01:06, 12.88it/s]\n",
      "Epoch 39: MAE/CA 11.604389: : 860it [01:06, 12.85it/s]\n",
      "Epoch 40: MAE/CA 11.677580: : 860it [01:08, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 2.7075e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: MAE/CA 11.194960: : 860it [01:06, 12.91it/s]\n",
      "Epoch 42: MAE/CA 11.395279: : 860it [01:07, 12.82it/s]\n",
      "Epoch 43: MAE/CA 11.799602: : 860it [01:07, 12.79it/s]\n",
      "Epoch 44: MAE/CA 11.703280: : 860it [01:07, 12.78it/s]\n",
      "Epoch 45: MAE/CA 11.046879: : 860it [01:06, 12.84it/s]\n",
      "Epoch 46: MAE/CA 11.037964: : 860it [01:06, 12.85it/s]\n",
      "Epoch 47: MAE/CA 10.983130: : 860it [01:06, 12.88it/s]\n",
      "Epoch 48: MAE/CA 11.185805: : 860it [01:07, 12.80it/s]\n",
      "Epoch 49: MAE/CA 11.178795: : 860it [01:08, 12.63it/s]\n",
      "Epoch 50: MAE/CA 13.010411: : 860it [01:07, 12.67it/s]\n",
      "Epoch 51: MAE/CA 12.212091: : 860it [01:07, 12.66it/s]\n",
      "Epoch 52: MAE/CA 12.035996: : 860it [01:08, 12.61it/s]\n",
      "Epoch 53: MAE/CA 11.374151: : 860it [01:07, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    63: reducing learning rate of group 0 to 2.5721e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: MAE/CA 11.177121: : 860it [01:07, 12.69it/s]\n",
      "Epoch 55: MAE/CA 10.895792: : 860it [01:11, 11.97it/s]\n",
      "Epoch 56: MAE/CA 10.840669: : 860it [01:15, 11.39it/s]\n",
      "Epoch 57: MAE/CA 12.272431: : 860it [01:07, 12.83it/s]\n",
      "Epoch 58: MAE/CA 12.009669: : 860it [00:59, 14.53it/s]\n",
      "Epoch 59: MAE/CA 11.249029: : 860it [00:59, 14.40it/s]\n",
      "Epoch 60: MAE/CA 10.568789: : 860it [01:00, 14.22it/s]\n",
      "Epoch 61: MAE/CA 10.609090: : 860it [01:02, 13.71it/s]\n",
      "Epoch 62: MAE/CA 10.409435: : 860it [01:07, 12.77it/s]\n",
      "Epoch 63: MAE/CA 10.924024: : 860it [01:07, 12.70it/s]\n",
      "Epoch 64: MAE/CA 11.340211: : 860it [01:07, 12.65it/s]\n",
      "Epoch 65: MAE/CA 10.792006: : 860it [01:07, 12.65it/s]\n",
      "Epoch 66: MAE/CA 10.790958: : 860it [01:07, 12.71it/s]\n",
      "Epoch 67: MAE/CA 10.186450: : 860it [01:07, 12.65it/s]\n",
      "Epoch 68: MAE/CA 9.985707: : 860it [01:07, 12.66it/s] \n",
      "Epoch 69: MAE/CA 10.582305: : 860it [01:07, 12.70it/s]\n",
      "Epoch 70: MAE/CA 10.287564: : 860it [01:07, 12.77it/s]\n",
      "Epoch 71: MAE/CA 10.474260: : 860it [01:07, 12.69it/s]\n",
      "Epoch 72: MAE/CA 11.314088: : 860it [01:07, 12.70it/s]\n",
      "Epoch 73: MAE/CA 10.756134: : 860it [01:08, 12.64it/s]\n",
      "Epoch 74: MAE/CA 10.474054: : 860it [01:07, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    84: reducing learning rate of group 0 to 2.4435e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: MAE/CA 10.395594: : 860it [01:08, 12.63it/s]\n",
      "Epoch 76: MAE/CA 10.268546: : 860it [01:08, 12.53it/s]\n",
      "Epoch 77: MAE/CA 10.187668: : 860it [01:08, 12.57it/s]\n",
      "Epoch 78: MAE/CA 10.092585: : 860it [01:07, 12.82it/s]\n",
      "Epoch 79: MAE/CA 10.092655: : 860it [01:08, 12.56it/s]\n",
      "Epoch 80: MAE/CA 9.707719: : 860it [01:07, 12.78it/s]\n",
      "Epoch 81: MAE/CA 10.367082: : 860it [01:07, 12.77it/s]\n",
      "Epoch 82: MAE/CA 10.247466: : 860it [01:07, 12.81it/s]\n",
      "Epoch 83: MAE/CA 10.134599: : 860it [01:06, 12.85it/s]\n",
      "Epoch 84: MAE/CA 11.052890: : 860it [01:07, 12.72it/s]\n",
      "Epoch 85: MAE/CA 10.329376: : 860it [01:06, 12.90it/s]\n",
      "Epoch 86: MAE/CA 9.652295: : 860it [01:07, 12.76it/s]\n",
      "Epoch 87: MAE/CA 9.586045: : 860it [01:07, 12.76it/s]\n",
      "Epoch 88: MAE/CA 9.604527: : 860it [01:07, 12.73it/s]\n",
      "Epoch 89: MAE/CA 9.720873: : 860it [01:06, 12.84it/s]\n",
      "Epoch 90: MAE/CA 9.884996: : 860it [01:07, 12.82it/s]\n",
      "Epoch 91: MAE/CA 10.020887: : 860it [01:07, 12.81it/s]\n",
      "Epoch 92: MAE/CA 9.884328: : 860it [01:06, 12.85it/s] \n",
      "Epoch 93: MAE/CA 9.967094: : 860it [01:07, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   103: reducing learning rate of group 0 to 2.3213e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: MAE/CA 9.814396: : 860it [01:06, 12.88it/s] \n",
      "Epoch 95: MAE/CA 9.644099: : 860it [01:06, 12.84it/s]\n",
      "Epoch 96: MAE/CA 9.479100: : 860it [01:06, 12.84it/s]\n",
      "Epoch 97: MAE/CA 9.576264: : 860it [01:07, 12.77it/s]\n",
      "Epoch 98: MAE/CA 9.324957: : 860it [01:05, 13.08it/s]\n",
      "Epoch 99: MAE/CA 9.514146: : 860it [01:09, 12.35it/s]\n",
      "Epoch 100: MAE/CA 9.507185: : 860it [01:07, 12.78it/s]\n",
      "Epoch 101: MAE/CA 9.607712: : 860it [01:14, 11.48it/s]\n",
      "Epoch 102: MAE/CA 9.195068: : 860it [01:16, 11.28it/s]\n",
      "Epoch 103: MAE/CA 9.415773: : 860it [01:10, 12.15it/s]\n",
      "Epoch 104: MAE/CA 9.411882: : 860it [01:00, 14.32it/s]\n",
      "Epoch 105: MAE/CA 9.184112: : 860it [01:04, 13.24it/s]\n",
      "Epoch 106: MAE/CA 9.093210: : 860it [01:06, 12.84it/s]\n",
      "Epoch 107: MAE/CA 9.060607: : 860it [01:07, 12.81it/s]\n",
      "Epoch 108: MAE/CA 9.186903: : 860it [01:07, 12.82it/s]\n",
      "Epoch 109: MAE/CA 8.883504: : 860it [01:06, 12.98it/s]\n",
      "Epoch 110: MAE/CA 8.961036: : 860it [01:07, 12.81it/s]\n",
      "Epoch 111: MAE/CA 8.904660: : 860it [01:07, 12.79it/s]\n",
      "Epoch 112: MAE/CA 9.205126: : 860it [01:06, 12.97it/s]\n",
      "Epoch 113: MAE/CA 8.881267: : 860it [01:06, 12.84it/s]\n",
      "Epoch 114: MAE/CA 9.014786: : 860it [01:06, 12.89it/s]\n",
      "Epoch 115: MAE/CA 9.051753: : 860it [01:06, 12.95it/s]\n",
      "Epoch 116: MAE/CA 9.043910: : 860it [01:06, 12.84it/s]\n",
      "Epoch 117: MAE/CA 8.774316: : 860it [01:06, 12.85it/s]\n",
      "Epoch 118: MAE/CA 8.613755: : 860it [01:08, 12.58it/s]\n",
      "Epoch 119: MAE/CA 8.911703: : 860it [01:07, 12.82it/s]\n",
      "Epoch 120: MAE/CA 9.045767: : 860it [01:06, 12.89it/s]\n",
      "Epoch 121: MAE/CA 8.868870: : 860it [01:07, 12.83it/s]\n",
      "Epoch 122: MAE/CA 8.788874: : 860it [01:07, 12.83it/s]\n",
      "Epoch 123: MAE/CA 8.520325: : 860it [01:02, 13.69it/s]\n",
      "Epoch 124: MAE/CA 8.941125: : 860it [00:56, 15.16it/s]\n",
      "Epoch 125: MAE/CA 8.703998: : 860it [00:56, 15.20it/s]\n",
      "Epoch 126: MAE/CA 8.856969: : 860it [00:57, 15.03it/s]\n",
      "Epoch 127: MAE/CA 8.618272: : 860it [00:57, 14.91it/s]\n",
      "Epoch 128: MAE/CA 8.603560: : 860it [00:57, 14.91it/s]\n",
      "Epoch 129: MAE/CA 8.538623: : 860it [00:59, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   139: reducing learning rate of group 0 to 2.2053e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130: MAE/CA 8.294313: : 860it [00:59, 14.44it/s]\n",
      "Epoch 131: MAE/CA 8.338695: : 860it [00:56, 15.19it/s]\n",
      "Epoch 132: MAE/CA 8.569461: : 860it [01:00, 14.16it/s]\n",
      "Epoch 133: MAE/CA 8.250676: : 860it [00:58, 14.59it/s]\n",
      "Epoch 134: MAE/CA 8.272770: : 860it [00:58, 14.61it/s]\n",
      "Epoch 135: MAE/CA 8.193269: : 860it [00:58, 14.61it/s]\n",
      "Epoch 136: MAE/CA 8.107286: : 860it [00:58, 14.60it/s]\n",
      "Epoch 137: MAE/CA 8.224704: : 860it [00:58, 14.61it/s]\n",
      "Epoch 138: MAE/CA 8.138376: : 860it [01:00, 14.30it/s]\n",
      "Epoch 139: MAE/CA 8.098367: : 860it [00:59, 14.54it/s]\n",
      "Epoch 140: MAE/CA 8.285211: : 860it [00:59, 14.48it/s]\n",
      "Epoch 141: MAE/CA 7.987214: : 860it [00:58, 14.61it/s]\n",
      "Epoch 142: MAE/CA 7.984403: : 860it [00:59, 14.36it/s]\n",
      "Epoch 143: MAE/CA 7.946938: : 860it [00:59, 14.46it/s]\n",
      "Epoch 144: MAE/CA 7.963986: : 860it [00:59, 14.56it/s]\n",
      "Epoch 145: MAE/CA 8.223763: : 860it [00:59, 14.54it/s]\n",
      "Epoch 146: MAE/CA 8.257481: : 860it [00:59, 14.52it/s]\n",
      "Epoch 147: MAE/CA 8.464110: : 860it [00:59, 14.52it/s]\n",
      "Epoch 148: MAE/CA 8.033912: : 860it [00:59, 14.48it/s]\n",
      "Epoch 149: MAE/CA 7.666331: : 860it [01:00, 14.10it/s]\n",
      "Epoch 150: MAE/CA 7.678557: : 860it [01:15, 11.41it/s]\n",
      "Epoch 151: MAE/CA 7.497720: : 860it [01:26,  9.94it/s]\n",
      "Epoch 152: MAE/CA 7.378217: : 860it [01:36,  8.93it/s]\n",
      "Epoch 153: MAE/CA 7.580366: : 860it [01:37,  8.86it/s]\n",
      "Epoch 154: MAE/CA 7.792419: : 860it [01:36,  8.87it/s]\n",
      "Epoch 155: MAE/CA 7.597997: : 860it [01:35,  8.97it/s]\n",
      "Epoch 156: MAE/CA 7.493576: : 860it [01:33,  9.22it/s]\n",
      "Epoch 157: MAE/CA 7.850849: : 860it [01:09, 12.37it/s]\n",
      "Epoch 158: MAE/CA 7.603519: : 860it [01:02, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   168: reducing learning rate of group 0 to 2.0950e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159: MAE/CA 7.844792: : 860it [01:14, 11.54it/s]\n",
      "Epoch 160: MAE/CA 7.995304: : 860it [01:17, 11.08it/s]\n",
      "Epoch 161: MAE/CA 7.630594: : 860it [01:16, 11.30it/s]\n",
      "Epoch 162: MAE/CA 7.590954: : 860it [01:15, 11.32it/s]\n",
      "Epoch 163: MAE/CA 7.435776: : 860it [01:16, 11.21it/s]\n",
      "Epoch 164: MAE/CA 7.499196: : 860it [01:15, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   174: reducing learning rate of group 0 to 1.9903e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165: MAE/CA 7.425297: : 860it [01:16, 11.25it/s]\n",
      "Epoch 166: MAE/CA 7.426188: : 860it [01:05, 13.14it/s]\n",
      "Epoch 167: MAE/CA 7.399407: : 860it [01:04, 13.27it/s]\n",
      "Epoch 168: MAE/CA 7.388739: : 860it [01:05, 13.06it/s]\n",
      "Epoch 169: MAE/CA 7.205552: : 860it [01:02, 13.76it/s]\n",
      "Epoch 170: MAE/CA 7.265141: : 860it [01:03, 13.64it/s]\n",
      "Epoch 171: MAE/CA 7.411328: : 860it [01:06, 13.00it/s]\n",
      "Epoch 172: MAE/CA 7.602295: : 860it [01:08, 12.57it/s]\n",
      "Epoch 173: MAE/CA 7.110268: : 860it [01:08, 12.48it/s]\n",
      "Epoch 174: MAE/CA 6.963226: : 860it [01:09, 12.42it/s]\n",
      "Epoch 175: MAE/CA 7.023363: : 860it [01:08, 12.53it/s]\n",
      "Epoch 176: MAE/CA 7.209128: : 860it [01:10, 12.20it/s]\n",
      "Epoch 177: MAE/CA 7.155317: : 860it [01:08, 12.48it/s]\n",
      "Epoch 178: MAE/CA 7.268568: : 860it [01:09, 12.44it/s]\n",
      "Epoch 179: MAE/CA 7.140032: : 860it [01:09, 12.37it/s]\n",
      "Epoch 180: MAE/CA 7.204929: : 860it [01:09, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   190: reducing learning rate of group 0 to 1.8907e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181: MAE/CA 7.036961: : 860it [01:09, 12.40it/s]\n",
      "Epoch 182: MAE/CA 6.935127: : 860it [01:07, 12.76it/s]\n",
      "Epoch 183: MAE/CA 6.848633: : 860it [01:10, 12.16it/s]\n",
      "Epoch 184: MAE/CA 6.801398: : 860it [01:08, 12.53it/s]\n",
      "Epoch 185: MAE/CA 6.862222: : 860it [01:08, 12.49it/s]\n",
      "Epoch 186: MAE/CA 6.760225: : 860it [01:09, 12.37it/s]\n",
      "Epoch 187: MAE/CA 6.860900: : 860it [01:09, 12.41it/s]\n",
      "Epoch 188: MAE/CA 6.805523: : 860it [01:09, 12.41it/s]\n",
      "Epoch 189: MAE/CA 6.795055: : 860it [01:09, 12.40it/s]\n",
      "Epoch 190: MAE/CA 6.849457: : 860it [01:09, 12.30it/s]\n",
      "Epoch 191: MAE/CA 6.877143: : 860it [01:10, 12.13it/s]\n",
      "Epoch 192: MAE/CA 7.015489: : 860it [01:09, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   202: reducing learning rate of group 0 to 1.7962e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193: MAE/CA 7.562793: : 860it [01:09, 12.42it/s]\n",
      "Epoch 194: MAE/CA 6.875301: : 860it [01:09, 12.41it/s]\n",
      "Epoch 195: MAE/CA 6.733657: : 860it [01:09, 12.41it/s]\n",
      "Epoch 196: MAE/CA 6.859292: : 860it [01:10, 12.20it/s]\n",
      "Epoch 197: MAE/CA 6.576509: : 860it [01:08, 12.48it/s]\n",
      "Epoch 198: MAE/CA 6.659021: : 860it [01:09, 12.45it/s]\n",
      "Epoch 199: MAE/CA 6.738879: : 860it [01:09, 12.33it/s]\n",
      "Epoch 200: MAE/CA 6.647497: : 860it [01:08, 12.50it/s]\n",
      "Epoch 201: MAE/CA 6.632392: : 860it [01:10, 12.25it/s]\n",
      "Epoch 202: MAE/CA 6.502913: : 860it [01:09, 12.43it/s]\n",
      "Epoch 203: MAE/CA 6.583929: : 860it [01:09, 12.29it/s]\n",
      "Epoch 204: MAE/CA 6.476869: : 860it [01:08, 12.57it/s]\n",
      "Epoch 205: MAE/CA 6.662472: : 860it [01:10, 12.21it/s]\n",
      "Epoch 206: MAE/CA 6.582253: : 860it [01:08, 12.62it/s]\n",
      "Epoch 207: MAE/CA 6.763137: : 860it [01:07, 12.71it/s]\n",
      "Epoch 208: MAE/CA 6.401024: : 860it [01:08, 12.50it/s]\n",
      "Epoch 209: MAE/CA 6.419488: : 860it [01:09, 12.38it/s]\n",
      "Epoch 210: MAE/CA 6.389044: : 860it [01:08, 12.47it/s]\n",
      "Epoch 211: MAE/CA 6.576048: : 860it [01:13, 11.76it/s]\n",
      "Epoch 212: MAE/CA 6.411095: : 860it [01:19, 10.77it/s]\n",
      "Epoch 213: MAE/CA 6.339175: : 860it [01:19, 10.80it/s]\n",
      "Epoch 214: MAE/CA 6.302587: : 860it [01:22, 10.43it/s]\n",
      "Epoch 215: MAE/CA 6.365126: : 860it [01:29,  9.60it/s]\n",
      "Epoch 216: MAE/CA 6.270139: : 860it [01:17, 11.11it/s]\n",
      "Epoch 217: MAE/CA 6.381509: : 860it [01:18, 11.02it/s]\n",
      "Epoch 218: MAE/CA 6.359000: : 860it [01:26, 10.00it/s]\n",
      "Epoch 219: MAE/CA 6.487758: : 860it [01:28,  9.68it/s]\n",
      "Epoch 220: MAE/CA 6.164108: : 860it [01:27,  9.80it/s]\n",
      "Epoch 221: MAE/CA 6.115214: : 860it [01:26,  9.99it/s]\n",
      "Epoch 222: MAE/CA 6.267687: : 860it [01:28,  9.69it/s]\n",
      "Epoch 223: MAE/CA 6.231504: : 860it [01:26,  9.93it/s]\n",
      "Epoch 224: MAE/CA 6.045625: : 860it [01:26,  9.93it/s]\n",
      "Epoch 225: MAE/CA 6.064645: : 860it [01:27,  9.81it/s]\n",
      "Epoch 226: MAE/CA 6.037974: : 860it [01:25, 10.08it/s]\n",
      "Epoch 227: MAE/CA 6.232007: : 860it [01:26,  9.96it/s]\n",
      "Epoch 228: MAE/CA 6.203240: : 860it [01:27,  9.83it/s]\n",
      "Epoch 229: MAE/CA 6.038029: : 860it [01:29,  9.65it/s]\n",
      "Epoch 230: MAE/CA 6.179346: : 860it [01:26,  9.93it/s]\n",
      "Epoch 231: MAE/CA 6.037080: : 860it [01:27,  9.86it/s]\n",
      "Epoch 232: MAE/CA 6.085493: : 860it [01:30,  9.53it/s]\n",
      "Epoch 233: MAE/CA 6.077151: : 860it [01:28,  9.71it/s]\n",
      "Epoch 234: MAE/CA 5.997863: : 860it [01:30,  9.51it/s]\n",
      "Epoch 235: MAE/CA 5.975660: : 860it [01:32,  9.31it/s]\n",
      "Epoch 236: MAE/CA 5.927314: : 860it [01:29,  9.59it/s]\n",
      "Epoch 237: MAE/CA 5.877536: : 860it [01:28,  9.75it/s]\n",
      "Epoch 238: MAE/CA 5.873301: : 860it [01:28,  9.72it/s]\n",
      "Epoch 239: MAE/CA 5.905065: : 860it [01:27,  9.79it/s]\n",
      "Epoch 240: MAE/CA 5.896181: : 860it [01:28,  9.76it/s]\n",
      "Epoch 241: MAE/CA 5.941316: : 860it [01:27,  9.80it/s]\n",
      "Epoch 242: MAE/CA 5.801319: : 860it [01:28,  9.70it/s]\n",
      "Epoch 243: MAE/CA 5.895198: : 860it [01:28,  9.74it/s]\n",
      "Epoch 244: MAE/CA 5.808001: : 860it [01:28,  9.77it/s]\n",
      "Epoch 245: MAE/CA 5.873936: : 860it [01:28,  9.69it/s]\n",
      "Epoch 246: MAE/CA 5.732084: : 860it [01:30,  9.53it/s]\n",
      "Epoch 247: MAE/CA 5.771876: : 860it [01:26,  9.91it/s]\n",
      "Epoch 248: MAE/CA 5.716499: : 860it [01:27,  9.82it/s]\n",
      "Epoch 249: MAE/CA 5.642472: : 860it [01:18, 10.92it/s]\n",
      "Epoch 250: MAE/CA 5.769612: : 860it [01:20, 10.65it/s]\n",
      "Epoch 251: MAE/CA 5.733641: : 860it [01:17, 11.15it/s]\n",
      "Epoch 252: MAE/CA 5.744252: : 860it [01:16, 11.20it/s]\n",
      "Epoch 253: MAE/CA 5.655573: : 860it [01:16, 11.24it/s]\n",
      "Epoch 254: MAE/CA 5.638931: : 860it [01:18, 10.93it/s]\n",
      "Epoch 255: MAE/CA 5.510732: : 217it [00:17, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard interrupt caught\n"
     ]
    }
   ],
   "source": [
    "min_mae = config.store_starting_from_ratio\n",
    "try:\n",
    "    # For each epoch\n",
    "    for epoch in range(config.n_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        pbar = tqdm.tqdm(enumerate(train_loader, 0))\n",
    "        epoch_loss = 0\n",
    "        for i, data in pbar:\n",
    "            model.zero_grad()\n",
    "            data = {key:d.to(device) for key, d in data.items()} \n",
    "            target = data['target']\n",
    "            prediction = model(data)\n",
    "            output = loss(prediction.view(-1), target.view(-1))\n",
    "            mae = (prediction.view(-1) - target.view(-1)).abs().mean()\n",
    "            epoch_loss = (epoch_loss*i + mae.item())/(i+1)\n",
    "            \n",
    "            pbar.set_description('Epoch %d: MAE/CA %2.6f'%(epoch+1, epoch_loss/chemical_accuracy[target_idx]))\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({'MSE': output.item()})\n",
    "        curr_lr = list(optimizer.param_groups)[0]['lr']\n",
    "        wandb.log({'MAE':epoch_loss, \n",
    "                   'MAE/CA':epoch_loss/chemical_accuracy[target_idx],\n",
    "                   'learning rate':curr_lr})\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        loss_hist += [epoch_loss] \n",
    "\n",
    "        if epoch_loss/chemical_accuracy[target_idx] < min_mae*config.required_improvement:\n",
    "            min_mae = epoch_loss/chemical_accuracy[target_idx]\n",
    "            torch.save(model.state_dict(), config.model_dir+'gtransformer_epoch%d.pt'%(epoch+1))\n",
    "        if curr_lr < config.minimal_lr:\n",
    "            break\n",
    "        if epoch_loss/chemical_accuracy[target_idx] < config.target_ratio:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt caught')\n",
    "    torch.save(model.state_dict(), config.model_dir+'gtransformer_epoch%d.pt'%(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "689f8c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "339it [00:04, 78.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3034960627555847 7.05804797106011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm.tqdm(enumerate(test_loader, 0))\n",
    "epoch_loss = 0\n",
    "maes = []\n",
    "for i, data in pbar:\n",
    "    data = {key:d.to(device) for key, d in data.items()} \n",
    "    target = data['target']\n",
    "    prediction = model(data)\n",
    "    mae = (prediction.view(-1) - target.view(-1)).abs()\n",
    "    maes += [mae.detach().cpu()]\n",
    "maes = torch.cat(maes, dim=0)\n",
    "mae = maes.mean().item()\n",
    "print(mae, mae/chemical_accuracy[target_idx])\n",
    "wandb.log({'TEST MAE':mae, 'TEST MAE/CA':mae/chemical_accuracy[target_idx]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb58439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
