{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a7ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0033798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import wandb\n",
    "import os\n",
    "import torch.optim as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7844993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfs_code\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a81b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No normalization for BCUT2D_MWHI\n",
      "WARNING:root:No normalization for BCUT2D_MWLOW\n",
      "WARNING:root:No normalization for BCUT2D_CHGHI\n",
      "WARNING:root:No normalization for BCUT2D_CHGLO\n",
      "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
      "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
      "WARNING:root:No normalization for BCUT2D_MRHI\n",
      "WARNING:root:No normalization for BCUT2D_MRLOW\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path = ['../../src'] + sys.path\n",
    "from dfs_transformer import EarlyStopping, DFSCodeSeq2SeqFC, smiles2graph, BERTize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a9aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfs_transformer import DFSCodeSeq2SeqFCFeatures, Trainer, PubChem, get_n_files\n",
    "from dfs_transformer.training.utils import seq_loss, seq_acc, collate_BERT, collate_rnd2min\n",
    "import argparse\n",
    "import yaml\n",
    "import functools\n",
    "from ml_collections import ConfigDict\n",
    "fname = '../../config/selfattn/bert10K.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585768c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname) as file:\n",
    "    config = ConfigDict(yaml.load(file, Loader=yaml.FullLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69384b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2021-09-15 19:42:23.968033: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/extras/CUPTI/lib64/:/opt/intel/lib:/opt/intel/mkl/lib/intel64:/opt/intel:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/bin/x86-64_linux:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/python/3.7/x86-64_linux:/opt/intel/clck_latest/lib:/opt/intel/daal/lib:/opt/intel/intelpython3/lib:/opt/intel/ipp/lib:/opt/intel/itac_2019/lib:/opt/intel/itac_latest/lib:/opt/intel/mkl/lib:/opt/intel/mkl_/lib:/opt/intel/mpirt/lib:/opt/intel/tbb/lib:/opt/intel/clck/2019.0/lib:/opt/intel/compilers_and_libraries_2019/linux/lib:/opt/intel/compilers_and_libraries/linux/lib:/opt/intel/itac/2019.0.018/lib:/opt/intel/itac_2019/intel64/lib:/opt/intel/itac_latest/intel64/lib:/opt/intel/parallel_studio_xe_2019.0.045/clck_2019/lib:/opt/intel/parallel_studio_xe_2019.0.045/itac_2019/lib:/opt/intel/parallel_studio_xe_2019/clck_2019/lib:/opt/intel/parallel_studio_xe_2019/itac_2019/lib:/opt/cuda/extras/CUPTI/lib64/:/opt/intel/lib:/opt/intel/mkl/lib/intel64:/opt/intel:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/bin/x86-64_linux:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/python/3.7/x86-64_linux:/opt/intel/clck_latest/lib:/opt/intel/daal/lib:/opt/intel/intelpython3/lib:/opt/intel/ipp/lib:/opt/intel/itac_2019/lib:/opt/intel/itac_latest/lib:/opt/intel/mkl/lib:/opt/intel/mkl_/lib:/opt/intel/mpirt/lib:/opt/intel/tbb/lib:/opt/intel/clck/2019.0/lib:/opt/intel/compilers_and_libraries_2019/linux/lib:/opt/intel/compilers_and_libraries/linux/lib:/opt/intel/itac/2019.0.018/lib:/opt/intel/itac_2019/intel64/lib:/opt/intel/itac_latest/intel64/lib:/opt/intel/parallel_studio_xe_2019.0.045/clck_2019/lib:/opt/intel/parallel_studio_xe_2019.0.045/itac_2019/lib:/opt/intel/parallel_studio_xe_2019/clck_2019/lib:/opt/intel/parallel_studio_xe_2019/itac_2019/lib\n",
      "2021-09-15 19:42:23.968055: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(mode=\"offline\", project=\"pubchem-experimental\", entity=\"chrisxx\", \n",
    "                 name=\"bert-10K\", config=config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a92cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = config.model\n",
    "t = config.training\n",
    "d = config.data\n",
    "device = torch.device('cuda:%d'%config.training.gpu_id if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ce = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "bce = nn.BCEWithLogitsLoss()    \n",
    "\n",
    "fields = ['acc-dfs1', 'acc-dfs2', 'acc-atm1', 'acc-atm2', 'acc-bnd']\n",
    "metrics = {field:functools.partial(seq_acc, idx=idx) for idx, field in enumerate(fields)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07de93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(dlist, fraction_missing=0.1):\n",
    "    node_batch = [] \n",
    "    edge_batch = []\n",
    "    min_code_batch = []\n",
    "    for d in dlist:\n",
    "        node_batch += [d.node_features]\n",
    "        edge_batch += [d.edge_features]\n",
    "        atom1 = d.node_features[d.min_dfs_code[:, -3]]\n",
    "        atom2 = d.node_features[d.min_dfs_code[:, -1]]\n",
    "        bond = d.edge_features[d.min_dfs_code[:, -2]]\n",
    "        min_code_batch += [torch.cat((d.min_dfs_code, atom1, atom2, bond), dim=1)]\n",
    "\n",
    "    inputs, outputs = BERTize(min_code_batch, fraction_missing=fraction_missing)\n",
    "    inputs = [inp[:, :8].long() for inp in inputs]\n",
    "    targets = nn.utils.rnn.pad_sequence(outputs, padding_value=-1)\n",
    "    return inputs, node_batch, edge_batch, targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab77d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, target):\n",
    "    dfs1, dfs2, atm1, atm2, bnd, feat = pred\n",
    "    \n",
    "    pred_dfs1 = torch.reshape(dfs1, (-1, m.max_nodes))\n",
    "    pred_dfs2 = torch.reshape(dfs2, (-1, m.max_nodes))\n",
    "    pred_atm1 = torch.reshape(atm1, (-1, m.n_atoms))\n",
    "    pred_atm2 = torch.reshape(atm2, (-1, m.n_atoms))\n",
    "    pred_bnd = torch.reshape(bnd, (-1, m.n_bonds))\n",
    "    pred_feat = torch.reshape(feat, (-1,  2*m.n_node_features + m.n_edge_features))\n",
    "    \n",
    "    tgt_dfs1 = target[:, :, 0].view(-1).long()\n",
    "    tgt_dfs2 = target[:, :, 1].view(-1).long()\n",
    "    tgt_atm1 = target[:, :, 2].view(-1).long()\n",
    "    tgt_atm2 = target[:, :, 4].view(-1).long()\n",
    "    tgt_bnd = target[:, :, 3].view(-1).long()\n",
    "    tgt_feat = target[:, :, 8:].view(-1, 2*m.n_node_features + m.n_edge_features)\n",
    "    \n",
    "    loss = ce(pred_dfs1, tgt_dfs1) \n",
    "    loss += ce(pred_dfs2, tgt_dfs2)\n",
    "    loss += ce(pred_atm1, tgt_atm1)\n",
    "    loss += ce(pred_bnd, tgt_bnd)\n",
    "    loss += ce(pred_atm2, tgt_atm2)\n",
    "    \n",
    "    mask = tgt_dfs1 != -1\n",
    "    loss += bce(pred_feat[mask], tgt_feat[mask])\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db77070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DFSCodeSeq2SeqFCFeatures(**m)\n",
    "    \n",
    "if t.load_last and t.es_path is not None:\n",
    "    model.load_state_dict(torch.load(t.es_path, map_location=device))\n",
    "elif t.pretrained_dir is not None:\n",
    "    model.load_state_dict(torch.load(t.pretrained_dir, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c51eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 16.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9978/9978 [00:00<00:00, 14780.83it/s]\n"
     ]
    }
   ],
   "source": [
    "validloader = None\n",
    "if d.valid_path is not None:\n",
    "    validset = PubChem('../.'+d.valid_path, max_nodes=m.max_nodes, max_edges=m.max_edges)\n",
    "    validloader = DataLoader(validset, batch_size=d.batch_size, shuffle=True, \n",
    "                             pin_memory=False, collate_fn=collate_fn)\n",
    "    exclude = validset.smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e687df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "783ff7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25192fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, None, loss, validloader=validloader, metrics=metrics, \n",
    "                  wandb_run = run, **t)\n",
    "trainer.n_epochs = d.n_iter_per_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "365b9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = get_n_files('../.'+d.path)\n",
    "if d.n_used is None:\n",
    "    n_splits = 1\n",
    "else:\n",
    "    n_splits = n_files // d.n_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 14.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9983/9983 [00:00<00:00, 14008.04it/s]\n",
      "Epoch 1: loss 10.721580 0.0417 0.0556 0.7222 0.7222 0.5556: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:42<00:00,  4.66it/s]\n",
      "Valid 1: loss 9.652095 0.0545 0.0909 0.6909 0.7636 0.5818: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.45it/s]\n",
      "Epoch 2: loss 9.462198 0.0349 0.0581 0.8605 0.6512 0.5233: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:43<00:00,  4.63it/s]\n",
      "Valid 2: loss 9.214743 0.0984 0.0328 0.8689 0.7869 0.6066: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.37it/s]\n",
      "Epoch 3: loss 8.594794 0.0909 0.1039 0.7922 0.7922 0.7403: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:44<00:00,  4.46it/s]\n",
      "Valid 3: loss 8.180081 0.1250 0.1071 0.7500 0.7857 0.6250: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.41it/s]\n",
      "Epoch 4: loss 7.921314 0.1899 0.1646 0.8608 0.8354 0.6456: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:43<00:00,  4.59it/s]\n",
      "Valid 4: loss 7.319169 0.2063 0.2381 0.8095 0.7619 0.6032: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.50it/s]\n",
      "Epoch 5: loss 7.542325 0.1802 0.2162 0.7838 0.7297 0.6486:  18%|████████████████████▌                                                                                             | 36/200 [00:07<00:35,  4.68it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(t.n_epochs):\n",
    "    print('starting epoch %d'%(epoch+1))\n",
    "    for split in range(n_splits):\n",
    "        dataset = PubChem('../.'+d.path, n_used = d.n_used, max_nodes=m.max_nodes, \n",
    "                          max_edges=m.max_edges, exclude=exclude)\n",
    "        loader = DataLoader(dataset, batch_size=d.batch_size, shuffle=True, \n",
    "                            pin_memory=False, collate_fn=collate_fn)\n",
    "        trainer.loader = loader\n",
    "        trainer.fit()\n",
    "        if trainer.stop_training:\n",
    "            break\n",
    "    if trainer.stop_training:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38336fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(torch.cat(d[1]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(torch.cat(d[2]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11de97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store config and model\n",
    "with open(trainer.es_path+'config.yaml', 'w') as f:\n",
    "    yaml.dump(config.to_dict(), f, default_flow_style=False)\n",
    "if args.name is not None and args.wandb_mode != \"offline\":\n",
    "    trained_model_artifact = wandb.Artifact(args.name, type=\"model\", description=\"trained selfattn model\")\n",
    "    trained_model_artifact.add_dir(trainer.es_path)\n",
    "    run.log_artifact(trained_model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c5af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
