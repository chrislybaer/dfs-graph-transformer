{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a7ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import wandb\n",
    "import os\n",
    "import torch.optim as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfs_code\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import copy\n",
    "torch.multiprocessing.set_sharing_strategy('file_system') # this is important\n",
    "# ulimit -n 500000\n",
    "def set_worker_sharing_strategy(worker_id: int) -> None:\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['/home/chrisw/Documents/projects/2021/graph-transformer/src'] + sys.path\n",
    "from dfs_transformer import EarlyStopping, DFSCodeSeq2SeqFC, Deepchem2TorchGeometric, collate_minc_rndc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69384b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='moleculenet', entity='chrisxx')\n",
    "\n",
    "config = wandb.config\n",
    "config.n_atoms = 118\n",
    "config.n_bonds = 4\n",
    "config.emb_dim = 120\n",
    "config.nhead = 12\n",
    "config.nlayers = 6\n",
    "config.max_nodes = 400\n",
    "config.max_edges = 600\n",
    "config.dim_feedforward = 2048\n",
    "config.lr = 0.0003\n",
    "config.n_epochs = 25\n",
    "config.patience = 3\n",
    "config.factor = 0.95\n",
    "config.minimal_lr = 6e-8\n",
    "config.target_idx = 7\n",
    "config.batch_size = 128\n",
    "config.valid_patience = 5\n",
    "config.valid_minimal_improvement = 0.00\n",
    "config.pretrained_dir = '../models/chembl/transformer/medium/'\n",
    "config.model_dir = '../../models/moleculenet/bbbp/medium/'\n",
    "config.data_dir = \"/mnt/ssd/datasets/ChEMBL/ChEMBL100_noH/\"\n",
    "config.num_workers = 4\n",
    "config.load_last = False\n",
    "config.dataset = 'clintox' # supported 'bbbp', 'clintox', 'hiv', 'tox21'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a388e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.get_all_sharing_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85546446",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = config.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b31fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cuda = lambda T: [t.cuda() for t in T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DFSCodeSeq2SeqFC(n_atoms=config.n_atoms,\n",
    "                         n_bonds=config.n_bonds, \n",
    "                         emb_dim=config.emb_dim, \n",
    "                         nhead=config.nhead, \n",
    "                         nlayers=config.nlayers, \n",
    "                         max_nodes=config.max_nodes, \n",
    "                         max_edges=config.max_edges,\n",
    "                         atom_encoder=nn.Embedding(config.n_atoms, config.emb_dim), \n",
    "                         bond_encoder=nn.Linear(config.n_bonds, config.emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_last:\n",
    "    model.load_state_dict(torch.load(config.pretrained_dir+'checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "if config.dataset == 'clintox':\n",
    "    tasks, datasets, transformers = dc.molnet.load_clintox(reload=False, featurizer=dc.feat.RawFeaturizer(True))\n",
    "elif config.dataset == 'tox21':\n",
    "    tasks, datasets, transformers = dc.molnet.load_tox21(reload=False, featurizer=dc.feat.RawFeaturizer(True))\n",
    "elif config.dataset == 'hiv':\n",
    "    tasks, datasets, transformers = dc.molnet.load_hiv(reload=False, featurizer=dc.feat.RawFeaturizer(True))\n",
    "elif config.dataset == 'bbbp':\n",
    "    tasks, datasets, transformers = dc.molnet.load_bbbp(reload=False, featurizer=dc.feat.RawFeaturizer(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head = nn.Linear(3*5*config.emb_dim, 1)\n",
    "model_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8eca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validset, testset = datasets\n",
    "collate_fn = collate_minc_rndc_y\n",
    "trainloader = DataLoader(Deepchem2TorchGeometric(trainset, -1), batch_size=config.batch_size, shuffle=True, pin_memory=False, collate_fn=collate_fn)\n",
    "validloader = DataLoader(Deepchem2TorchGeometric(validset, -1), batch_size=config.batch_size, shuffle=False, pin_memory=False, collate_fn=collate_fn)\n",
    "testloader = DataLoader(Deepchem2TorchGeometric(testset, -1), batch_size=config.batch_size, shuffle=False, pin_memory=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optimizers.Adam(list(model.parameters()) + list(model_head.parameters()), lr=config.lr)\n",
    "\n",
    "lr_scheduler = optimizers.lr_scheduler.ReduceLROnPlateau(optim, mode='min', verbose=True, patience=config.patience, factor=config.factor)\n",
    "early_stopping_model = EarlyStopping(patience=config.valid_patience, delta=config.valid_minimal_improvement,\n",
    "                              path=config.model_dir+'checkpoint_model.pt')\n",
    "early_stopping_head = EarlyStopping(patience=config.valid_patience, delta=config.valid_minimal_improvement,\n",
    "                              path=config.model_dir+'checkpoint_head.pt')\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04629fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def score(loader, model, model_head, use_min=False):\n",
    "    val_roc = 0\n",
    "    with torch.no_grad():\n",
    "        full_preds, target = [], []\n",
    "        for batch in tqdm.tqdm(loader):\n",
    "            rndc, minc, z, eattr, y = batch\n",
    "            if use_min:\n",
    "                code = to_cuda(minc)\n",
    "            else:\n",
    "                code = to_cuda(rndc)\n",
    "            y = y.to(device)\n",
    "            features = model.encode(code, to_cuda(z), to_cuda(eattr)) # not clear whether to use minc or randc\n",
    "            pred = sigmoid(model_head(features)).squeeze()\n",
    "            \n",
    "            pred_np = pred.detach().cpu().numpy().tolist()\n",
    "            y_np = y.detach().cpu().numpy().tolist()\n",
    "            full_preds += pred_np\n",
    "            target += y_np\n",
    "\n",
    "        target = np.asarray(target)\n",
    "        full_preds = np.asarray(full_preds)\n",
    "        roc = roc_auc_score(target, full_preds)\n",
    "        prc = average_precision_score(target, full_preds)\n",
    "    return roc, prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df794b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores = []\n",
    "try:\n",
    "    for epoch in range(config.n_epochs):  \n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm.tqdm(trainloader)\n",
    "        model.train()\n",
    "        for i, data in enumerate(pbar):\n",
    "            optim.zero_grad()\n",
    "            rndc, minc, z, eattr, y = data\n",
    "            rndc = to_cuda(rndc)\n",
    "            z = to_cuda(z)\n",
    "            eattr = to_cuda(eattr)\n",
    "            y = y.to(device)\n",
    "            #prediction\n",
    "            features = model.encode(rndc, z, eattr)\n",
    "            prediction = model_head(features).squeeze()\n",
    "            loss = bce(prediction, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optim.step()\n",
    "            epoch_loss = (epoch_loss*i + loss.item())/(i+1)\n",
    "            \n",
    "            wandb.log({'loss':epoch_loss})\n",
    "            pbar.set_description('Epoch %d: CE %2.6f'%(epoch+1, epoch_loss))\n",
    "        model.eval()\n",
    "        roc_auc_valid, prc_auc_valid = score(validloader, model, model_head) \n",
    "        valid_scores += [roc_auc_valid]\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        early_stopping_model(-roc_auc_valid, model)\n",
    "        early_stopping_head(-roc_auc_valid, model_head)\n",
    "        curr_lr = list(optim.param_groups)[0]['lr']\n",
    "        wandb.log({'roc_valid':roc_auc_valid, 'prc_valid':prc_auc_valid, 'learning rate':curr_lr})\n",
    "        print('ROCAUC',roc_auc_valid, 'PRCAUC', prc_auc_valid)\n",
    "\n",
    "        if early_stopping_model.early_stop:\n",
    "            break\n",
    "\n",
    "        if curr_lr < config.minimal_lr:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), config.model_dir+'_keyboardinterrupt.pt')\n",
    "    print('keyboard interrupt caught')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(config.model_dir+'checkpoint_model.pt'))\n",
    "model_head.load_state_dict(torch.load(config.model_dir+'checkpoint_head.pt'))\n",
    "model.eval()\n",
    "roc_auc_valid, prc_auc_valid = score(testloader, model, model_head) \n",
    "wandb.log({'roc_test':roc_auc_valid, 'prc_test':prc_auc_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC, PRC VALID', score_min(validloader, model, model_head, True))\n",
    "print('ROC, PRC TEST', score_min(testloader, model, model_head, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e4cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef9b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
