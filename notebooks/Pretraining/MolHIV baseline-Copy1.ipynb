{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672a0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import _dfs_codes as dfs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ogb.graphproppred import Evaluator\n",
    "from ogb.graphproppred import GraphPropPredDataset\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import dfs_code\n",
    "import networkx as nx\n",
    "\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56d59c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  44\n"
     ]
    }
   ],
   "source": [
    "from dfs_transformer import EarlyStopping\n",
    "import wandb\n",
    "import random \n",
    "import os\n",
    "\n",
    "manualSeed = 44\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "print(\"Random Seed: \", manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259538f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrisxx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ethereal-armadillo-17</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chrisxx/molhiv-transformer\" target=\"_blank\">https://wandb.ai/chrisxx/molhiv-transformer</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chrisxx/molhiv-transformer/runs/2ewxhmgx\" target=\"_blank\">https://wandb.ai/chrisxx/molhiv-transformer/runs/2ewxhmgx</a><br/>\n",
       "                Run data is saved locally in <code>/home/chrisw/Documents/projects/2021/graph-transformer/notebooks/Pretraining/wandb/run-20210817_141241-2ewxhmgx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='molhiv-transformer', entity='chrisxx')\n",
    "config = wandb.config\n",
    "config.nlayers = 6\n",
    "config.emb_dim = 50\n",
    "config.nhead = 5\n",
    "config.lr = 0.00005\n",
    "config.n_epochs = 1000\n",
    "config.patience = 5\n",
    "config.factor = 0.9\n",
    "config.minimal_lr = 6e-8\n",
    "config.target_idx = 7\n",
    "config.batch_size = 128\n",
    "config.valid_patience = 20\n",
    "config.valid_minimal_improvement=0.00\n",
    "config.model_dir = '../models/molhiv/transformer/1/'\n",
    "config.num_workers = 4\n",
    "config.dfs_codes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b768cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ba33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplittedDataset(Dataset):\n",
    "    def __init__(self, D, S):\n",
    "        super().__init__()\n",
    "        self.D, self.S = D, S\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.S)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.D[self.S[idx]]\n",
    "\n",
    "\n",
    "class MolhivDFSCodeDataset(Dataset):\n",
    "    def __init__(self, max_nodes=40):\n",
    "        super().__init__()\n",
    "        self.maxN = max_nodes\n",
    "        self.molhiv = GraphPropPredDataset(name='ogbg-molhiv')\n",
    "        self.throw_labels = lambda code: list(map(lambda c: c[:2] + c[-3:], code))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.molhiv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        added_e, Edges = [], []\n",
    "        G, L = self.molhiv[idx]\n",
    "        \n",
    "        edge_index = []\n",
    "        elabels = []\n",
    "        efeats = []\n",
    "        for edge, feat in zip(G['edge_index'].T, G['edge_feat']):\n",
    "            if edge[0] < self.maxN and edge[1] < self.maxN:\n",
    "                edge_index.append(edge.tolist())\n",
    "                elabels.append(feat[0])\n",
    "                efeats.append(feat.tolist())\n",
    "        edge_index = np.asarray(edge_index).astype(np.int64).T\n",
    "        efeats = np.asarray(efeats)\n",
    "        vlabels = G['node_feat'][:self.maxN, 0].tolist()\n",
    "        \n",
    "        # only keep largest connected component\n",
    "        edges_coo = edge_index.copy().T\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(np.arange(len(vlabels)))\n",
    "        g.add_edges_from(edges_coo.tolist())\n",
    "\n",
    "        ccs = list(nx.connected_components(g))\n",
    "        largest_cc = ccs[np.argmax([len(cc) for cc in ccs])]\n",
    "        node_ids = np.asarray(list(largest_cc))\n",
    "\n",
    "        x = G['node_feat'][:min(G['num_nodes'], self.maxN)][node_ids]\n",
    "        z = x[:, 0]\n",
    "        edges_cc = []\n",
    "        edge_feats = []\n",
    "        edge_labels = []\n",
    "        old2new = {old:new for new, old in enumerate(node_ids)}\n",
    "        for idx, (u, v) in enumerate(edges_coo):\n",
    "            if u in node_ids and v in node_ids:\n",
    "                edges_cc += [[old2new[u], old2new[v]]]\n",
    "                edge_feats += [efeats[idx].tolist()]\n",
    "                edge_labels += [elabels[idx]]\n",
    "        edge_index = torch.tensor(edges_cc, dtype=torch.long).T\n",
    "        edge_attr = torch.tensor(edge_feats, dtype=torch.int32)\n",
    "\n",
    "\n",
    "        data = Data(x=x, z=z, pos=None, edge_index=edge_index,\n",
    "                    edge_attr=edge_attr, y=None)\n",
    "        \n",
    "        \n",
    "        #Edges, _ = dfs_code.min_dfs_code_from_torch_geometric(data, z.tolist(), edge_labels)\n",
    "        Edges, _ = dfs_code.rnd_dfs_code_from_torch_geometric(data, z.tolist(), edge_labels)\n",
    "        return torch.LongTensor(Edges), torch.tensor(x, dtype=torch.int32), edge_attr, L[0] \n",
    "    #C (code) #N (node features) #E (edge features) and label\n",
    "\n",
    "\n",
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=160):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x * math.sqrt(self.d_model)\n",
    "            seq_len = x.size(1)\n",
    "            pe = self.pe[:, :seq_len]\n",
    "            x = x + pe\n",
    "            return x\n",
    "    \n",
    "\n",
    "class DFSCodeTransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, nhead, max_nodes, nlayers):\n",
    "        super().__init__()\n",
    "        self.ninp = emb_dim * 5\n",
    "        self.fc_out = nn.Linear(self.ninp, 1)\n",
    "        self.dfs_enc = nn.Embedding(max_nodes, emb_dim) # this should be maxN instead of 200\n",
    "        self.PE = PositionalEncoder(self.ninp, max_nodes)\n",
    "        self.atom_enc = AtomEncoder(emb_dim=emb_dim)\n",
    "        self.bond_enc = BondEncoder(emb_dim=emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, self.ninp), requires_grad=True)\n",
    "        self.enc = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.ninp, nhead=nhead), nlayers)\n",
    "\n",
    "        nn.init.normal_(self.cls_token, mean=.0, std=.5)\n",
    "\n",
    "    def prepare_tokens(self, C, N, E):\n",
    "        src = []\n",
    "        for code, n_feats, e_feats in zip(C, N, E):\n",
    "            atom_emb, bond_emb = self.atom_enc(n_feats), self.bond_enc(e_feats)\n",
    "            dfs_emb = self.dfs_enc(code[:, :2].flatten()).reshape(len(code), -1)\n",
    "            dfs_emb *= 0 #TODO: undo me\n",
    "            src.append(torch.cat((dfs_emb, atom_emb[code[:, -3]], bond_emb[code[:, -2]], atom_emb[code[:, -1]]), dim=1))\n",
    "\n",
    "        batch = self.PE(nn.utils.rnn.pad_sequence(src))\n",
    "        return torch.cat((self.cls_token.expand(-1, batch.shape[1], -1), batch), dim=0)\n",
    "\n",
    "    def forward(self, C, N, E):\n",
    "        self_attn = self.enc(self.prepare_tokens(C, N, E) * math.sqrt(self.ninp))\n",
    "        return self.fc_out(self_attn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = MolhivDFSCodeDataset(max_nodes=400)\n",
    "data_split = D.molhiv.get_idx_split()\n",
    "valdata = [D[i] for i in data_split['valid']]\n",
    "testdata = [D[i] for i in data_split['test']]\n",
    "traindata = [D[i] for i in data_split['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3398e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(valdata, batch_size=config.batch_size, pin_memory=True, collate_fn=lambda x:x)\n",
    "test_loader = DataLoader(testdata, batch_size=config.batch_size//2, pin_memory=True, collate_fn=lambda x:x)\n",
    "train_loader = DataLoader(traindata, batch_size=config.batch_size//2, pin_memory=True, shuffle=True, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df06e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_h, val_h = [], []\n",
    "evaluator = Evaluator(name='ogbg-molhiv')\n",
    "to_cuda = lambda T: map(lambda t: t.cuda(), T)\n",
    "\n",
    "\n",
    "M = DFSCodeTransformer(emb_dim=config.emb_dim, nhead=config.nhead, max_nodes=D.maxN, nlayers=config.nlayers).cuda()\n",
    "optim = optimizers.Adam(M.parameters(), lr=config.lr)#, betas=(0.9, 0.98))#, eps=1e-7)\n",
    "\n",
    "lr_scheduler = optimizers.lr_scheduler.ReduceLROnPlateau(optim, mode='min', verbose=True, patience=config.patience, factor=config.factor)\n",
    "early_stopping = EarlyStopping(patience=config.valid_patience, delta=config.valid_minimal_improvement,\n",
    "                              path=config.model_dir+'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(alpha=2, gamma=5)\n",
    "criterion = nn.BCELoss(weight=torch.FloatTensor([0.5, 14.5]))\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(loader, model):\n",
    "    M = model\n",
    "    val_roc = 0\n",
    "    with torch.no_grad():\n",
    "        full_preds, target = [], []\n",
    "        for batch in tqdm(loader):\n",
    "            C, N, E, y = zip(*batch)\n",
    "            y = torch.Tensor(y).cuda()\n",
    "            pred = M(to_cuda(C), to_cuda(N), to_cuda(E)).squeeze()\n",
    "            target.extend(y.cpu().tolist())\n",
    "            full_preds.extend((1. * (0.5 < torch.sigmoid(pred))).cpu().tolist())\n",
    "\n",
    "        val_roc = evaluator.eval({'y_true': np.expand_dims(target, axis=1),\n",
    "                                  'y_pred': np.expand_dims(full_preds, axis=1)})['rocauc']\n",
    "    return val_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c87454",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(config.n_epochs):  \n",
    "        M.train()\n",
    "        tot_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "\n",
    "            C, N, E, y = zip(*batch)\n",
    "            y = torch.Tensor(y).cuda()\n",
    "            pred = M(to_cuda(C), to_cuda(N), to_cuda(E)).squeeze()\n",
    "            optim.zero_grad()\n",
    "            #loss = nn.BCELoss(weight=14*y+.5)(torch.sigmoid(pred), y) #weight=14*y+.5\n",
    "            #loss = criterion(torch.sigmoid(pred), y)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "        M.eval()\n",
    "        val_roc = score(val_loader, M)\n",
    "\n",
    "        epoch += 1\n",
    "        val_h.append(val_roc)\n",
    "        loss_h.append(tot_loss / len(data_split['train']))\n",
    "\n",
    "        lr_scheduler.step(tot_loss)\n",
    "        early_stopping(-val_roc, M)\n",
    "        curr_lr = list(optim.param_groups)[0]['lr']\n",
    "\n",
    "\n",
    "        wandb.log({'BCE':loss_h[-1], \n",
    "                   'ROCAUC valid':val_roc,\n",
    "                   'learning rate':curr_lr})\n",
    "\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "        if curr_lr < config.minimal_lr:\n",
    "            break\n",
    "\n",
    "        print(f'\\nepoch: {len(loss_h)} - loss: {loss_h[-1]} - val: {val_h[-1]}', flush=True)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt caught')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "M.load_state_dict(torch.load(config.model_dir+'checkpoint.pt'))\n",
    "M.eval()\n",
    "\n",
    "test_roc = score(test_loader, M)\n",
    "\n",
    "\n",
    "print(f'\\ntest AUROC: {test_roc} at best val epoch {np.argmax(val_h)+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c14795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
