{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a7ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0033798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-27 14:57:42.236096: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/cuda/extras/CUPTI/lib64/:/opt/intel/lib:/opt/intel/mkl/lib/intel64:/opt/intel:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/bin/x86-64_linux:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/python/3.7/x86-64_linux:/opt/intel/clck_latest/lib:/opt/intel/daal/lib:/opt/intel/intelpython3/lib:/opt/intel/ipp/lib:/opt/intel/itac_2019/lib:/opt/intel/itac_latest/lib:/opt/intel/mkl/lib:/opt/intel/mkl_/lib:/opt/intel/mpirt/lib:/opt/intel/tbb/lib:/opt/intel/clck/2019.0/lib:/opt/intel/compilers_and_libraries_2019/linux/lib:/opt/intel/compilers_and_libraries/linux/lib:/opt/intel/itac/2019.0.018/lib:/opt/intel/itac_2019/intel64/lib:/opt/intel/itac_latest/intel64/lib:/opt/intel/parallel_studio_xe_2019.0.045/clck_2019/lib:/opt/intel/parallel_studio_xe_2019.0.045/itac_2019/lib:/opt/intel/parallel_studio_xe_2019/clck_2019/lib:/opt/intel/parallel_studio_xe_2019/itac_2019/lib:/opt/cuda/extras/CUPTI/lib64/:/opt/intel/lib:/opt/intel/mkl/lib/intel64:/opt/intel:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/bin/x86-64_linux:/opt/ibm/ILOG/CPLEX_Studio1210/cplex/python/3.7/x86-64_linux:/opt/intel/clck_latest/lib:/opt/intel/daal/lib:/opt/intel/intelpython3/lib:/opt/intel/ipp/lib:/opt/intel/itac_2019/lib:/opt/intel/itac_latest/lib:/opt/intel/mkl/lib:/opt/intel/mkl_/lib:/opt/intel/mpirt/lib:/opt/intel/tbb/lib:/opt/intel/clck/2019.0/lib:/opt/intel/compilers_and_libraries_2019/linux/lib:/opt/intel/compilers_and_libraries/linux/lib:/opt/intel/itac/2019.0.018/lib:/opt/intel/itac_2019/intel64/lib:/opt/intel/itac_latest/intel64/lib:/opt/intel/parallel_studio_xe_2019.0.045/clck_2019/lib:/opt/intel/parallel_studio_xe_2019.0.045/itac_2019/lib:/opt/intel/parallel_studio_xe_2019/clck_2019/lib:/opt/intel/parallel_studio_xe_2019/itac_2019/lib\n",
      "2021-08-27 14:57:42.236141: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import wandb\n",
    "import os\n",
    "import torch.optim as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7844993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfs_code\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import tqdm\n",
    "import copy\n",
    "#torch.multiprocessing.set_sharing_strategy('file_system') # this is important\n",
    "# ulimit -n 500000\n",
    "#def set_worker_sharing_strategy(worker_id: int) -> None:\n",
    "#    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a81b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['/home/chrisw/Documents/projects/2021/graph-transformer/src'] + sys.path\n",
    "from dfs_transformer import EarlyStopping, DFSCodeSeq2SeqFC, Deepchem2TorchGeometric, collate_minc_rndc_y, DFSCodeSeq2SeqFCLegacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69384b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project='moleculenet', entity='chrisxx')\n",
    "\n",
    "config = wandb.config\n",
    "config.n_atoms = 118\n",
    "config.n_bonds = 4\n",
    "config.emb_dim = 120\n",
    "config.nhead = 12\n",
    "config.nlayers = 6\n",
    "config.max_nodes = 400\n",
    "config.max_edges = 600\n",
    "config.max_nodes_data = 400\n",
    "config.max_edges_data = 600\n",
    "config.onlyRandom = True\n",
    "config.use_min = False # interestingly random seems to work better than using the minimal codes...\n",
    "config.dim_feedforward = 2048\n",
    "config.lr = 0.00003 # 10x smaller than the learning rate of the pretraining seems to be good\n",
    "config.clip_gradient = 0.5\n",
    "config.n_epochs = 25\n",
    "config.patience = 3\n",
    "config.factor = 0.95\n",
    "config.minimal_lr = 6e-8\n",
    "config.target_idx = 7\n",
    "config.batch_size = 64\n",
    "config.valid_patience = 5\n",
    "config.valid_minimal_improvement = 0.00\n",
    "config.pretrained_dir = '../../models/chembl/better_transformer/medium/leq1/'\n",
    "config.num_workers = 0\n",
    "config.load_last = True\n",
    "config.dataset = 'bbbp' # supported 'bbbp', 'clintox', 'hiv', 'tox21'\n",
    "config.model_dir = '../../models/moleculenet/%s/'%config.dataset\n",
    "config.seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7a30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ef93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a388e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_descriptor', 'file_system'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiprocessing.get_all_sharing_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b31fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9f1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cuda = lambda T: [t.cuda() for t in T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bae3e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DFSCodeSeq2SeqFC(n_atoms=config.n_atoms,\n",
    "                         n_bonds=config.n_bonds, \n",
    "                         emb_dim=config.emb_dim, \n",
    "                         nhead=config.nhead, \n",
    "                         nlayers=config.nlayers, \n",
    "                         max_nodes=config.max_nodes, \n",
    "                         max_edges=config.max_edges,\n",
    "                         atom_encoder=nn.Embedding(config.n_atoms, config.emb_dim), \n",
    "                         bond_encoder=nn.Linear(config.n_bonds, config.emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ad67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_last:\n",
    "    model.load_state_dict(torch.load(config.pretrained_dir+'checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f748be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DFSCodeSeq2SeqFC(\n",
       "  (encoder): DFSCodeEncoder(\n",
       "    (emb_dfs): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (emb_seq): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (emb_atom): Embedding(118, 120)\n",
       "    (emb_bond): Linear(in_features=4, out_features=120, bias=True)\n",
       "    (mixer): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (enc): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
       "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
       "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
       "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
       "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
       "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
       "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_dfs_idx1): Linear(in_features=1200, out_features=400, bias=True)\n",
       "  (fc_dfs_idx2): Linear(in_features=1200, out_features=400, bias=True)\n",
       "  (fc_atom1): Linear(in_features=1200, out_features=118, bias=True)\n",
       "  (fc_atom2): Linear(in_features=1200, out_features=118, bias=True)\n",
       "  (fc_bond): Linear(in_features=1200, out_features=4, bias=True)\n",
       "  (fc_eos): Linear(in_features=1200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed501ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_scaffold_split(dataset):\n",
    "    splitter = dc.splits.ScaffoldSplitter()\n",
    "    shuffled = dataset.complete_shuffle()\n",
    "    trainidx, valididx, testidx = splitter.split(shuffled)\n",
    "    train = shuffled.select(trainidx)\n",
    "    valid = shuffled.select(valididx)\n",
    "    test = shuffled.select(testidx)\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d927e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 59, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 61, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 391, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 614, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 642, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 645, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 646, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 647, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 648, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 649, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "Failed to featurize datapoint 685, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True)\n",
      "/home/chrisw/.cache/pypoetry/virtualenvs/graph-transformer-9jPERXQ--py3.8/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "if config.dataset == 'clintox':\n",
    "    tasks, datasets, transformers = dc.molnet.load_clintox(reload=False, featurizer=dc.feat.RawFeaturizer(True))\n",
    "elif config.dataset == 'tox21':\n",
    "    tasks, datasets, transformers = dc.molnet.load_tox21(reload=False, featurizer=dc.feat.RawFeaturizer(True))\n",
    "elif config.dataset == 'hiv':\n",
    "    tasks, datasets, transformers = dc.molnet.load_hiv(reload=False, featurizer=dc.feat.RawFeaturizer(True))\n",
    "elif config.dataset == 'bbbp':\n",
    "    tasks, datasets, transformers = dc.molnet.load_bbbp(reload=False, featurizer=dc.feat.RawFeaturizer(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "130deabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_head = nn.Linear(5*config.emb_dim, 1)\n",
    "model_head.to(device)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b0360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.dataset == 'tox21' or config.dataset == 'hiv' and not config.onlyRandom:\n",
    "    if config.dataset == 'tox21':\n",
    "        datadir = '../../datasets/moleculenet/tox21/srp53/'\n",
    "    if config.dataset == 'hiv':\n",
    "        datadir = '../../datasets/moleculenet/hiv_%d_%d/'%(config.max_nodes_data, config.max_edges_data)\n",
    "    collate_fn = collate_minc_rndc_y\n",
    "    trainloader = DataLoader(torch.load(datadir+'trainingset.pt'), batch_size=config.batch_size, shuffle=True, pin_memory=False, collate_fn=collate_fn)\n",
    "    validloader = DataLoader(torch.load(datadir+'validationset.pt'), batch_size=config.batch_size, shuffle=False, pin_memory=False, collate_fn=collate_fn)\n",
    "    testloader = DataLoader(torch.load(datadir+'testset.pt'), batch_size=config.batch_size, shuffle=False, pin_memory=False, collate_fn=collate_fn)\n",
    "else:\n",
    "    trainset, validset, testset = datasets\n",
    "    collate_fn = collate_minc_rndc_y\n",
    "    traindata = Deepchem2TorchGeometric(trainset, -1, onlyRandom=config.onlyRandom, max_nodes=config.max_nodes_data, max_edges=config.max_edges_data)\n",
    "    validdata = Deepchem2TorchGeometric(validset, -1, onlyRandom=config.onlyRandom, max_nodes=config.max_nodes_data, max_edges=config.max_edges_data)\n",
    "    testdata = Deepchem2TorchGeometric(testset, -1, onlyRandom=config.onlyRandom, max_nodes=config.max_nodes_data, max_edges=config.max_edges_data)\n",
    "    trainloader = DataLoader(traindata, batch_size=config.batch_size, shuffle=True, pin_memory=False, collate_fn=collate_fn, num_workers=config.num_workers)\n",
    "    validloader = DataLoader(validdata, batch_size=config.batch_size, shuffle=False, pin_memory=False, collate_fn=collate_fn, num_workers=config.num_workers)\n",
    "    testloader = DataLoader(testdata, batch_size=config.batch_size, shuffle=False, pin_memory=False, collate_fn=collate_fn, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cee73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optimizers.Adam(list(model.parameters()) + list(model_head.parameters()), lr=config.lr)\n",
    "\n",
    "lr_scheduler = optimizers.lr_scheduler.ReduceLROnPlateau(optim, mode='min', verbose=True, patience=config.patience, factor=config.factor)\n",
    "early_stopping_model = EarlyStopping(patience=config.valid_patience, delta=config.valid_minimal_improvement,\n",
    "                              path=config.model_dir+'checkpoint_model.pt')\n",
    "early_stopping_head = EarlyStopping(patience=config.valid_patience, delta=config.valid_minimal_improvement,\n",
    "                              path=config.model_dir+'checkpoint_head.pt')\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "158cd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def score(loader, model, model_head, use_min=False):\n",
    "    val_roc = 0\n",
    "    with torch.no_grad():\n",
    "        full_preds, target = [], []\n",
    "        for batch in tqdm.tqdm(loader):\n",
    "            rndc, minc, z, eattr, y = batch\n",
    "            if use_min:\n",
    "                code = to_cuda(minc)\n",
    "            else:\n",
    "                code = to_cuda(rndc)\n",
    "            y = y.to(device)\n",
    "            self_attn, features, eos = model.encode(code, to_cuda(z), to_cuda(eattr)) # not clear whether to use minc or randc\n",
    "            pred = sigmoid(model_head(features)).squeeze()\n",
    "            \n",
    "            pred_np = pred.detach().cpu().numpy().tolist()\n",
    "            y_np = y.detach().cpu().numpy().tolist()\n",
    "            full_preds += pred_np\n",
    "            target += y_np\n",
    "\n",
    "        target = np.asarray(target)\n",
    "        full_preds = np.asarray(full_preds)\n",
    "        roc = roc_auc_score(target, full_preds)\n",
    "        prc = average_precision_score(target, full_preds)\n",
    "    return roc, prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df794b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                       | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function AddmmBackward returned an invalid gradient at index 2 - got [5, 120] but expected shape compatible with [4, 120]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_869776/4109390746.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_gradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/graph-transformer-9jPERXQ--py3.8/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/graph-transformer-9jPERXQ--py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function AddmmBackward returned an invalid gradient at index 2 - got [5, 120] but expected shape compatible with [4, 120]"
     ]
    }
   ],
   "source": [
    "valid_scores = []\n",
    "try:\n",
    "    for epoch in range(config.n_epochs):  \n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm.tqdm(trainloader)\n",
    "        model.train()\n",
    "        for i, data in enumerate(pbar):\n",
    "            optim.zero_grad()\n",
    "            rndc, minc, z, eattr, y = data\n",
    "            if config.use_min:\n",
    "                code = to_cuda(minc)\n",
    "            else:\n",
    "                code = to_cuda(rndc)\n",
    "            z = to_cuda(z)\n",
    "            eattr = to_cuda(eattr)\n",
    "            y = y.to(device)\n",
    "            #prediction\n",
    "            self_attn, features, eos = model.encode(code, z, eattr)\n",
    "            prediction = model_head(features).squeeze()\n",
    "            loss = bce(prediction, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            if config.clip_gradient is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_gradient)\n",
    "            optim.step()\n",
    "            epoch_loss = (epoch_loss*i + loss.item())/(i+1)\n",
    "            \n",
    "            wandb.log({'loss':epoch_loss})\n",
    "            pbar.set_description('Epoch %d: CE %2.6f'%(epoch+1, epoch_loss))\n",
    "        model.eval()\n",
    "        roc_auc_valid, prc_auc_valid = score(validloader, model, model_head, config.use_min) \n",
    "        valid_scores += [roc_auc_valid]\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        early_stopping_model(-roc_auc_valid, model)\n",
    "        early_stopping_head(-roc_auc_valid, model_head)\n",
    "        curr_lr = list(optim.param_groups)[0]['lr']\n",
    "        wandb.log({'roc_valid':roc_auc_valid, 'prc_valid':prc_auc_valid, 'learning rate':curr_lr})\n",
    "        print('ROCAUC',roc_auc_valid, 'PRCAUC', prc_auc_valid)\n",
    "\n",
    "        if early_stopping_model.early_stop:\n",
    "            break\n",
    "\n",
    "        if curr_lr < config.minimal_lr:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), config.model_dir+'_keyboardinterrupt.pt')\n",
    "    print('keyboard interrupt caught')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(config.model_dir+'checkpoint_model.pt'))\n",
    "model_head.load_state_dict(torch.load(config.model_dir+'checkpoint_head.pt'))\n",
    "model.eval()\n",
    "roc_auc_valid, prc_auc_valid = score(testloader, model, model_head) \n",
    "wandb.log({'roc_test':roc_auc_valid, 'prc_test':prc_auc_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69962626",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc, prc = score(testloader, model, model_head, True)\n",
    "wandb.log({'roc_test_min':roc, 'prc_test_min':prc})\n",
    "print('ROC, PRC VALID', score(validloader, model, model_head, True))\n",
    "print('ROC, PRC TEST', score(testloader, model, model_head, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d30e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_roc = 0\n",
    "avg_prc = 0\n",
    "for i in range(20):\n",
    "    roc, prc = score(testloader, model, model_head, False)\n",
    "    avg_roc += roc\n",
    "    avg_prc += prc\n",
    "avg_roc /= 20\n",
    "avg_prc /= 20 \n",
    "wandb.log({'roc_test_avg20':avg_roc, 'prc_test_avg20':avg_prc})\n",
    "print('ROC, PRC TEST', avg_roc, avg_prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d00c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
