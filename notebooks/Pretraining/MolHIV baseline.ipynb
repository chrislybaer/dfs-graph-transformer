{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850f5cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "NetworkXError",
     "evalue": "Edge tuple [0, 1, 1, 2, 1, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 7, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 16, 18, 18, 19, 13, 20, 20, 21, 21, 22, 19, 4, 18, 6, 15, 10, 22, 12] must be a 2-tuple or 3-tuple.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_255046/121513028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_255046/121513028.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_255046/121513028.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/graph-transformer-9jPERXQ--py3.8/lib/python3.8/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# doesn't need edge_attr_dict_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Edge tuple {e} must be a 2-tuple or 3-tuple.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNetworkXError\u001b[0m: Edge tuple [0, 1, 1, 2, 1, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 7, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 16, 18, 18, 19, 13, 20, 20, 21, 21, 22, 19, 4, 18, 6, 15, 10, 22, 12] must be a 2-tuple or 3-tuple."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import _dfs_codes as dfs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ogb.graphproppred import Evaluator\n",
    "from ogb.graphproppred import GraphPropPredDataset\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import dfs_code\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "class SplittedDataset(Dataset):\n",
    "    def __init__(self, D, S):\n",
    "        super().__init__()\n",
    "        self.D, self.S = D, S\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.S)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.D[self.S[idx]]\n",
    "\n",
    "\n",
    "class MolhivDFSCodeDataset(Dataset):\n",
    "    def __init__(self, max_nodes=40):\n",
    "        super().__init__()\n",
    "        self.maxN = max_nodes\n",
    "        self.molhiv = GraphPropPredDataset(name='ogbg-molhiv')\n",
    "        self.throw_labels = lambda code: list(map(lambda c: c[:2] + c[-3:], code))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.molhiv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        added_e, Edges = [], []\n",
    "        G, L = self.molhiv[idx]\n",
    "        \n",
    "        edge_index = []\n",
    "        elabels = []\n",
    "        efeats = []\n",
    "        for edge, feat in zip(G['edge_index'].T, G['edge_feat']):\n",
    "            if edge[0] < self.maxN and edge[1] < self.maxN:\n",
    "                edge_index.append(edge.tolist())\n",
    "                elabels.append(np.argmax(feat))\n",
    "                efeats.append(feat.tolist())\n",
    "        edge_index = np.asarray(edge_index).astype(np.int64).T\n",
    "        efeats = np.asarray(efeats)\n",
    "        vlabels = G['node_feat'][:self.maxN, 0].tolist()\n",
    "        \n",
    "        # only keep largest connected component\n",
    "        edges_coo = edge_index.copy()\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(np.arange(len(vlabels)))\n",
    "        g.add_edges_from(edges_coo.tolist())\n",
    "\n",
    "        ccs = list(nx.connected_components(g))\n",
    "        largest_cc = ccs[np.argmax([len(cc) for cc in ccs])]\n",
    "        node_ids = np.asarray(list(largest_cc))\n",
    "\n",
    "        x = G['node_feat'][:min(G['num_nodes'], self.maxN)][node_ids]\n",
    "        z = x[:, 0]\n",
    "        edges_cc = []\n",
    "        edge_feats = []\n",
    "        edge_labels = []\n",
    "        old2new = {old:new for new, old in enumerate(node_ids)}\n",
    "        for idx, (u, v) in enumerate(edges_coo):\n",
    "            if u in node_ids and v in node_ids:\n",
    "                edges_cc += [[old2new[u], old2new[v]]]\n",
    "                edge_feats += [efeats[idx]]\n",
    "                edge_labels += [elabels[idx]]\n",
    "        edge_index = torch.tensor(edges_cc, dtype=torch.long).T\n",
    "        edge_attr = torch.tensor(edge_feats, dtype=torch.float)\n",
    "\n",
    "\n",
    "        data = Data(x=x, z=z, pos=None, edge_index=edge_index,\n",
    "                    edge_attr=edge_attr, y=None)\n",
    "        \n",
    "        \n",
    "        Edges, _ = self.throw_labels(dfs_code.min_dfs_code_from_torch_geometric(data, z.tolist(), edge_labels))\n",
    "        return torch.LongTensor(Edges), \\\n",
    "    torch.IntTensor(G['node_feat'][:min(G['num_nodes'], self.maxN)]), \\\n",
    "    torch.IntTensor(efeats), L[0]\n",
    "\n",
    "\n",
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=160):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x * math.sqrt(self.d_model)\n",
    "            seq_len = x.size(1)\n",
    "            pe = self.pe[:, :seq_len]\n",
    "            x = x + pe\n",
    "            return x\n",
    "    \n",
    "\n",
    "class DFSCodeTransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, nhead):\n",
    "        super().__init__()\n",
    "        self.ninp = emb_dim * 5\n",
    "        self.fc_out = nn.Linear(self.ninp, 1)\n",
    "        self.dfs_enc = nn.Embedding(200, emb_dim)\n",
    "        self.PE = PositionalEncoder(self.ninp, 200)\n",
    "        self.atom_enc = AtomEncoder(emb_dim=emb_dim)\n",
    "        self.bond_enc = BondEncoder(emb_dim=emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, self.ninp), requires_grad=True)\n",
    "        self.enc = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.ninp, nhead=nhead), 6)\n",
    "\n",
    "        nn.init.normal_(self.cls_token, mean=.0, std=.5)\n",
    "\n",
    "    def prepare_tokens(self, C, N, E):\n",
    "        src = []\n",
    "        for code, n_feats, e_feats in zip(C, N, E):\n",
    "            atom_emb, bond_emb = self.atom_enc(n_feats), self.bond_enc(e_feats)\n",
    "            dfs_emb = self.dfs_enc(code[:, :2].flatten()).reshape(len(code), -1)\n",
    "            src.append(torch.cat((dfs_emb, atom_emb[code[:, 2]], bond_emb[code[:, 3]], atom_emb[code[:, 4]]), dim=1))\n",
    "\n",
    "        batch = self.PE(nn.utils.rnn.pad_sequence(src))\n",
    "        return torch.cat((self.cls_token.expand(-1, batch.shape[1], -1), batch), dim=0)\n",
    "\n",
    "    def forward(self, C, N, E):\n",
    "        self_attn = self.enc(self.prepare_tokens(C, N, E) * math.sqrt(self.ninp))\n",
    "        return self.fc_out(self_attn[0])\n",
    "\n",
    "\n",
    "epoch, patience = 0, 5\n",
    "loss_h, val_h = [], []\n",
    "evaluator = Evaluator(name='ogbg-molhiv')\n",
    "to_cuda = lambda T: map(lambda t: t.cuda(), T)\n",
    "\n",
    "D = MolhivDFSCodeDataset()\n",
    "data_split = D.molhiv.get_idx_split()\n",
    "M = DFSCodeTransformer(emb_dim=50, nhead=5).cuda()\n",
    "optim = optimizers.Adam(M.parameters(), lr=1e-7, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "val_loader = DataLoader([D[i] for i in data_split['valid']], batch_size=128, pin_memory=True, collate_fn=lambda x:x)\n",
    "test_loader = DataLoader([D[i] for i in data_split['test']], batch_size=128, pin_memory=True, collate_fn=lambda x:x)\n",
    "train_loader = DataLoader([D[i] for i in data_split['train']], batch_size=16, pin_memory=True, shuffle=True, collate_fn=lambda x:x)\n",
    "\n",
    "while epoch < patience or epoch - np.argmax(val_h) < patience:\n",
    "  \n",
    "    M.train()\n",
    "    tot_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        C, N, E, y = zip(*batch)\n",
    "        y = torch.Tensor(y).cuda()\n",
    "        pred = M(to_cuda(C), to_cuda(N), to_cuda(E)).squeeze()\n",
    "        optim.zero_grad()\n",
    "        loss = nn.BCELoss(weight=14*y+.5)(torch.sigmoid(pred), y) #weight=1\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        tot_loss += loss.item()\n",
    "    \n",
    "    M.eval()\n",
    "    val_roc = 0\n",
    "    with torch.no_grad():\n",
    "        full_preds, target = [], []\n",
    "        for batch in tqdm(val_loader):\n",
    "            C, N, E, y = zip(*batch)\n",
    "            y = torch.Tensor(y).cuda()\n",
    "            pred = M(to_cuda(C), to_cuda(N), to_cuda(E)).squeeze()\n",
    "            target.extend(y.cpu().tolist())\n",
    "            full_preds.extend((1. * (0.5 < torch.sigmoid(pred))).cpu().tolist())\n",
    "\n",
    "        val_roc = evaluator.eval({'y_true': np.expand_dims(target, axis=1),\n",
    "                                  'y_pred': np.expand_dims(full_preds, axis=1)})['rocauc']\n",
    "\n",
    "    epoch += 1\n",
    "    val_h.append(val_roc)\n",
    "    loss_h.append(tot_loss / len(data_split['train']))\n",
    "    if max(val_h) == val_roc: torch.save(M.state_dict(), 'molhiv_checkpoint.pt')\n",
    "    print(f'\\nepoch: {len(loss_h)} - loss: {loss_h[-1]} - val: {val_h[-1]}', flush=True)\n",
    "\n",
    "\n",
    "M.load_state_dict(torch.load('molhiv_checkpoint.pt'))\n",
    "M.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_preds, target = [], []\n",
    "    for batch in tqdm(test_loader):\n",
    "        C, N, E, y = zip(*batch)\n",
    "        y = torch.Tensor(y).cuda()\n",
    "        pred = M(to_cuda(C), to_cuda(N), to_cuda(E)).squeeze()\n",
    "        target.extend(y.cpu().tolist())\n",
    "        full_preds.extend((1. * (0.5 < torch.sigmoid(pred))).cpu().tolist())\n",
    "\n",
    "    test_roc = evaluator.eval({'y_true': np.expand_dims(target, axis=1),\n",
    "                               'y_pred': np.expand_dims(full_preds, axis=1)})['rocauc']\n",
    "\n",
    "print(f'\\ntest AUROC: {test_roc} at best val epoch {np.argmax(val_h)+1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf1395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
