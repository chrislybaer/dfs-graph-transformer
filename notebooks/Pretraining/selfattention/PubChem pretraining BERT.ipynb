{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a7ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import wandb\n",
    "import os\n",
    "import torch.optim as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfs_code\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "#torch.multiprocessing.set_sharing_strategy('file_system') # this is important on local machine\n",
    "#def set_worker_sharing_strategy(worker_id: int) -> None:\n",
    "#    torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['../../../src'] + sys.path\n",
    "from dfs_transformer import EarlyStopping, DFSCodeSeq2SeqFC, smiles2graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69384b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='pubchem-bert', entity='chrisxx', name='bert10M')\n",
    "\n",
    "config = wandb.config\n",
    "config.mode = \"min2min\" #rnd2rnd\n",
    "config.fraction_missing = 0.15\n",
    "config.n_atoms = 118\n",
    "config.n_bonds = 5\n",
    "config.emb_dim = 120\n",
    "config.nhead = 12\n",
    "config.nlayers = 6\n",
    "config.max_nodes = 250\n",
    "config.max_edges = 500\n",
    "config.dim_feedforward = 2048\n",
    "config.n_files = 64\n",
    "config.n_splits = 16\n",
    "config.n_iter_per_split = 1\n",
    "config.lr = 0.0003\n",
    "config.n_epochs = 10000\n",
    "config.lr_adjustment_period = 500\n",
    "config.patience = 5\n",
    "config.factor = 0.96\n",
    "config.minimal_lr = 6e-8\n",
    "config.batch_size = 100\n",
    "config.accumulate_grads = 1\n",
    "config.valid_patience = 10000\n",
    "config.valid_minimal_improvement=0.00\n",
    "config.model_dir = \"../../../models/pubchem10M/features_selfattention/medium/\"\n",
    "#config.data_dir = \"/mnt/project/pubchem_noH/\"\n",
    "config.data_dir = \"/home/wendlerc/noH/timeout60_4\"\n",
    "config.pretrained_dir = None#\"../../models/chembl/better_transformer/medium/\"\n",
    "config.num_workers = 0\n",
    "config.prefetch_factor = 2\n",
    "config.persistent_workers = False\n",
    "config.load_last = False\n",
    "config.dformat = \"pkl\"\n",
    "config.gpu_id = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85546446",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = config.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f04196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubChem(Dataset):\n",
    "    \"\"\"PubChem dataset of molecules and minimal DFS codes.\"\"\"\n",
    "    def __init__(self, path=path, n_used = 8, n_splits = 64, max_nodes=config.max_nodes,\n",
    "                 max_edges=config.max_edges, useHs=False, addLoops=False, memoryEfficient=False,\n",
    "                 transform=None, n_mols_per_dataset=np.inf, dformat='json'):\n",
    "        self.path = path\n",
    "        self.data = []\n",
    "        self.path = path\n",
    "        self.n_used = n_used\n",
    "        self.n_splits = n_splits\n",
    "        self.useHs = useHs\n",
    "        self.addLoops = addLoops\n",
    "        self.n_mols_per_dataset = n_mols_per_dataset\n",
    "        self.max_nodes = max_nodes\n",
    "        self.max_edges = max_edges\n",
    "        self.dformat = dformat\n",
    "        self.prepare()\n",
    "        \n",
    "        \n",
    "    def prepare(self):\n",
    "        codes_all = {}\n",
    "        d_all = {}\n",
    "        i2didx = {}\n",
    "        perm = np.random.permutation(self.n_splits)\n",
    "        for i in tqdm.tqdm(perm[:self.n_used]):\n",
    "            dname = glob.glob(self.path+\"/%d/min_dfs_codes_split*.json\"%(i+1))[0]\n",
    "            didx = int(dname.split(\"split\")[-1][:-5])\n",
    "            dname2 = self.path+\"/%d/data_split%d.%s\"%(i+1, didx, self.dformat)\n",
    "            with open(dname, 'r') as f:\n",
    "                codes = json.load(f)\n",
    "                for key, val in codes.items():\n",
    "                    codes_all[key] = val\n",
    "            if self.dformat == \"json\":\n",
    "                with open(dname2, 'r') as f:\n",
    "                    d_dict = json.load(f)\n",
    "                    for key, val in d_dict.items():\n",
    "                        d_all[key] = val\n",
    "            elif self.dformat == \"pkl\":\n",
    "                with open(dname2, 'rb') as f:\n",
    "                    d_dict = pickle.load(f)\n",
    "                    for key, val in d_dict.items():\n",
    "                        d_all[key] = val\n",
    "            else:\n",
    "                raise ValueError(\"unsupported dformat\")\n",
    "        \n",
    "        for smiles, code in tqdm.tqdm(codes_all.items()):\n",
    "            if len(self.data) > self.n_mols_per_dataset:\n",
    "                break\n",
    "            if code['min_dfs_code'] is not None and len(code['min_dfs_code']) > 1:\n",
    "                d = d_all[smiles]\n",
    "                if len(d['z']) > self.max_nodes:\n",
    "                    continue\n",
    "                if len(d['edge_attr']) > 2*self.max_edges:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                z = torch.tensor(d['z'], dtype=torch.long)\n",
    "                \n",
    "                data_ = Data(z=z,\n",
    "                             edge_attr=torch.tensor(d['edge_attr']),\n",
    "                             edge_index=torch.tensor(d['edge_index'], dtype=torch.long),\n",
    "                             min_dfs_code=torch.tensor(code['min_dfs_code']),\n",
    "                             min_dfs_index=torch.tensor(code['dfs_index'], dtype=torch.long),\n",
    "                             smiles=smiles,\n",
    "                             node_features=torch.tensor(d['atom_features'], dtype=torch.float32),\n",
    "                             edge_features=torch.tensor(d['bond_features'], dtype=torch.float32))\n",
    "                self.data += [data_]   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c51eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_features(dlist):\n",
    "    node_batch = [] \n",
    "    edge_batch = []\n",
    "    rnd_code_batch = []\n",
    "    min_code_batch = []\n",
    "    for d in dlist:\n",
    "        rnd_code, rnd_index = dfs_code.rnd_dfs_code_from_torch_geometric(d, \n",
    "                                                                         d.z.numpy().tolist(), \n",
    "                                                                         np.argmax(d.edge_attr.numpy(), axis=1))\n",
    "        node_batch += [d.node_features]\n",
    "        edge_batch += [d.edge_features]\n",
    "        rnd_code_batch += [torch.tensor(rnd_code)]\n",
    "        min_code_batch += [d.min_dfs_code]\n",
    "    return rnd_code_batch, node_batch, edge_batch, min_code_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b31fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device('cuda:%d'%config.gpu_id if (torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cuda = lambda T: [t.to(device) for t in T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bd256",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PubChem(n_used = 1, n_splits=config.n_files, dformat=config.dformat)\n",
    "loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, pin_memory=False, collate_fn=collate_fn_features,\n",
    "                   num_workers=config.num_workers, prefetch_factor=config.prefetch_factor, \n",
    "                    persistent_workers=config.persistent_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node_features = data[1][0].shape[1]\n",
    "n_edge_features = data[2][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69651ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_node_features, n_edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ba8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6196e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERTize(codes):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for code in codes:\n",
    "        n = len(code)\n",
    "        perm = np.random.permutation(n)\n",
    "        target_idx = perm[:int(config.fraction_missing*n)]\n",
    "        input_idx = perm[int(config.fraction_missing*n):]\n",
    "        inp = code.clone()\n",
    "        target = code.clone()\n",
    "        target[input_idx] = -1\n",
    "        inp[target_idx] = -1\n",
    "        inputs += [inp]\n",
    "        targets += [target]\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcef1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = BERTize(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf37fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17958009",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DFSCodeSeq2SeqFC(n_atoms=config.n_atoms,\n",
    "                         n_bonds=config.n_bonds, \n",
    "                         emb_dim=config.emb_dim, \n",
    "                         nhead=config.nhead, \n",
    "                         nlayers=config.nlayers, \n",
    "                         max_nodes=config.max_nodes, \n",
    "                         max_edges=config.max_edges,\n",
    "                         atom_encoder=nn.Linear(n_node_features, config.emb_dim), \n",
    "                         bond_encoder=nn.Linear(n_edge_features, config.emb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45977ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pretrained_dir is not None:\n",
    "    model.load_state_dict(torch.load(config.pretrained_dir+'checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_last:\n",
    "    model.load_state_dict(torch.load(config.model_dir+'checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optimizers.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "lr_scheduler = optimizers.lr_scheduler.ReduceLROnPlateau(optim, mode='min', verbose=True, patience=config.patience, factor=config.factor)\n",
    "#lr_scheduler = optimizers.lr_scheduler.ExponentialLR(optim, gamma=config.factor)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=config.valid_patience, delta=config.valid_minimal_improvement,\n",
    "                              path=config.model_dir+'checkpoint.pt')\n",
    "bce = torch.nn.BCEWithLogitsLoss()\n",
    "ce = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "softmax = nn.Softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df794b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(config.n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for split in range(config.n_splits):\n",
    "            \n",
    "            n_ids = config.n_files//config.n_splits\n",
    "            dataset = PubChem(n_used = n_ids, n_splits = config.n_files, dformat=config.dformat)\n",
    "            loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, pin_memory=False, \n",
    "                                collate_fn=collate_fn_features,\n",
    "                   num_workers=config.num_workers, prefetch_factor=config.prefetch_factor, \n",
    "                    persistent_workers=config.persistent_workers)\n",
    "            for j in range(config.n_iter_per_split):\n",
    "                pbar = tqdm.tqdm(loader)\n",
    "                for i, data in enumerate(pbar):\n",
    "                    if i % config.accumulate_grads == 0: #bei 0 wollen wir das\n",
    "                        optim.zero_grad()\n",
    "                    rndc, nattr, eattr, minc = data\n",
    "                    if config.mode == \"min2min\":\n",
    "                        code = to_cuda(minc)\n",
    "                    elif config.mode == \"rnd2rnd\":\n",
    "                        code = to_cuda(rndc)\n",
    "                    else:\n",
    "                        raise ValueError(\"unrecognized config.mode\")\n",
    "                    nattr = to_cuda(nattr)\n",
    "                    eattr = to_cuda(eattr)\n",
    "                    #prepare inputs\n",
    "                    inputs, targets = BERTize(code)\n",
    "                    #prepare labels\n",
    "                    targetc = [torch.cat((c, (-1)*torch.ones((1, 8), dtype=torch.long, device=device)), dim=0) for c in targets]\n",
    "                    targetc_seq = nn.utils.rnn.pad_sequence(targetc, padding_value=-1)\n",
    "                    #prediction\n",
    "                    dfs1, dfs2, atm1, atm2, bnd, eos = model(inputs, nattr, eattr)\n",
    "                    pred_dfs1 = torch.reshape(dfs1, (-1, config.max_nodes))\n",
    "                    pred_dfs2 = torch.reshape(dfs2, (-1, config.max_nodes))\n",
    "                    pred_atm1 = torch.reshape(atm1, (-1, config.n_atoms))\n",
    "                    pred_atm2 = torch.reshape(atm2, (-1, config.n_atoms))\n",
    "                    pred_bnd = torch.reshape(bnd, (-1, config.n_bonds))\n",
    "                    tgt_dfs1 = targetc_seq[:, :, 0].view(-1)\n",
    "                    tgt_dfs2 = targetc_seq[:, :, 1].view(-1)\n",
    "                    tgt_atm1 = targetc_seq[:, :, 2].view(-1)\n",
    "                    tgt_atm2 = targetc_seq[:, :, 4].view(-1)\n",
    "                    tgt_bnd = targetc_seq[:, :, 3].view(-1)\n",
    "                    loss = ce(pred_dfs1, tgt_dfs1) \n",
    "                    loss += ce(pred_dfs2, tgt_dfs2)\n",
    "                    loss += ce(pred_atm1, tgt_atm1)\n",
    "                    loss += ce(pred_bnd, tgt_bnd)\n",
    "                    loss += ce(pred_atm2, tgt_atm2)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                    if (i+1) % config.accumulate_grads == 0:\n",
    "                        optim.step() # bei 2 wollen wir das\n",
    "                    epoch_loss = (epoch_loss*i + loss.item())/(i+1)\n",
    "                    mask = tgt_dfs1 != -1\n",
    "                    n_tgts = torch.sum(mask)\n",
    "                    acc_dfs1 = (torch.argmax(pred_dfs1[mask], axis=1) == tgt_dfs1[mask]).sum()/n_tgts\n",
    "                    acc_dfs2 = (torch.argmax(pred_dfs2[mask], axis=1) == tgt_dfs2[mask]).sum()/n_tgts\n",
    "                    acc_atm1 = (torch.argmax(pred_atm1[mask], axis=1) == tgt_atm1[mask]).sum()/n_tgts\n",
    "                    acc_atm2 = (torch.argmax(pred_atm2[mask], axis=1) == tgt_atm2[mask]).sum()/n_tgts\n",
    "                    acc_bnd = (torch.argmax(pred_bnd[mask], axis=1) == tgt_bnd[mask]).sum()/n_tgts\n",
    "                    curr_lr = list(optim.param_groups)[0]['lr']\n",
    "                    wandb.log({'loss':epoch_loss, 'learning rate':curr_lr,\n",
    "                               'acc-dfs1':acc_dfs1, 'acc-dfs2':acc_dfs2, \n",
    "                               'acc-atm1':acc_atm1, 'acc-atm2':acc_atm2,\n",
    "                               'acc-bnd':acc_bnd})\n",
    "                    pbar.set_description('Epoch %d: CE %2.6f accs: %2.2f %2.2f %2.2f %2.2f %2.2f'%(epoch+1, \n",
    "                                                                                                   epoch_loss, \n",
    "                                                                                                   100*acc_dfs1,\n",
    "                                                                                                   100*acc_dfs2,\n",
    "                                                                                                   100*acc_atm1,\n",
    "                                                                                                   100*acc_bnd,\n",
    "                                                                                                   100*acc_atm2))\n",
    "\n",
    "                    if i % config.lr_adjustment_period == 0:\n",
    "                        early_stopping(epoch_loss, model)\n",
    "                        lr_scheduler.step(epoch_loss)\n",
    "                        if early_stopping.early_stop:\n",
    "                            break\n",
    "\n",
    "                        if curr_lr < config.minimal_lr:\n",
    "                            break\n",
    "                \n",
    "                \n",
    "            del dataset\n",
    "            del loader\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), config.model_dir+'_keyboardinterrupt.pt')\n",
    "    print('keyboard interrupt caught')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25192fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
