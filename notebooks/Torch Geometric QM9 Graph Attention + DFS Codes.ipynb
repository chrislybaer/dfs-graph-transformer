{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff921ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets.qm9 import QM9\n",
    "import torch_geometric.datasets.qm9 as qm9\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch_geometric.nn as tgnn\n",
    "from torch_scatter import scatter\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8b07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] Reports MAE in eV / Chemical Accuracy of the target variable U0. \n",
    "# The chemical accuracy of U0 is 0.043 see [1, Table 5].\n",
    "\n",
    "# Reproduced table [0]\n",
    "# MXMNet: 0.00590/0.043 = 0.13720930232558143\n",
    "# HMGNN:  0.00592/0.043 = 0.13767441860465118\n",
    "# MPNN:   0.01935/0.043 = 0.45\n",
    "# KRR:    0.0251 /0.043 = 0.5837209302325582\n",
    "# [0] https://paperswithcode.com/sota/formation-energy-on-qm9\n",
    "# [1] Neural Message Passing for Quantum Chemistry, https://arxiv.org/pdf/1704.01212v2.pdf\n",
    "# MXMNet https://arxiv.org/pdf/2011.07457v1.pdf\n",
    "# HMGNN https://arxiv.org/pdf/2009.12710v1.pdf\n",
    "# MPNN https://arxiv.org/pdf/1704.01212v2.pdf\n",
    "# KRR HDAD kernel ridge regression https://arxiv.org/pdf/1702.05532.pdf\n",
    "# HDAD means HDAD (Histogram of distances, anglesand dihedral angles)\n",
    "\n",
    "# [2] Reports the average value of MAE / Chemical Accuracy of over all targets\n",
    "# [2] https://paperswithcode.com/sota/drug-discovery-on-qm9\n",
    "target_dict = {0: 'mu, D, Dipole moment', \n",
    "               1: 'alpha, {a_0}^3, Isotropic polarizability', \n",
    "               2: 'epsilon_{HOMO}, eV, Highest occupied molecular orbital energy',\n",
    "               3: 'epsilon_{LUMO}, eV, Lowest unoccupied molecular orbital energy',\n",
    "               4: 'Delta, eV, Gap between HOMO and LUMO',\n",
    "               5: '< R^2 >, {a_0}^2, Electronic spatial extent',\n",
    "               6: 'ZPVE, eV, Zero point vibrational energy', \n",
    "               7: 'U_0, eV, Internal energy at 0K',\n",
    "               8: 'U, eV, Internal energy at 298.15K', \n",
    "               9: 'H, eV, Enthalpy at 298.15K',\n",
    "               10: 'G, eV, Free energy at 298.15K',  \n",
    "               11: 'c_{v}, cal\\(mol K), Heat capacity at 298.15K'}\n",
    "\n",
    "chemical_accuracy = {idx:0.043 for idx in range(12)}\n",
    "chemical_accuracy[0] = 0.1\n",
    "chemical_accuracy[1] = 0.1\n",
    "chemical_accuracy[5] = 1.2\n",
    "chemical_accuracy[6] = 0.0012\n",
    "chemical_accuracy[11] = 0.050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrisxx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.11.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">radiant-armadillo-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chrisxx/QM9-GAT\" target=\"_blank\">https://wandb.ai/chrisxx/QM9-GAT</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chrisxx/QM9-GAT/runs/1ujyy6j6\" target=\"_blank\">https://wandb.ai/chrisxx/QM9-GAT/runs/1ujyy6j6</a><br/>\n",
       "                Run data is saved locally in <code>/home/chrisw/Documents/projects/2021/graph-transformer/notebooks/wandb/run-20210720_140136-1ujyy6j6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='QM9-GAT', entity='chrisxx')\n",
    "config = wandb.config\n",
    "config.hidden_dim = 256\n",
    "config.nlayers = 6\n",
    "config.nhead = 1\n",
    "config.lr = 0.0003\n",
    "config.n_epochs = 5000\n",
    "config.patience = 5\n",
    "config.factor = 0.95\n",
    "config.minimal_lr = 6e-8\n",
    "config.target_idx = 7\n",
    "config.batch_size = 128\n",
    "config.n_train = 110000\n",
    "config.n_valid = 10000\n",
    "config.target_ratio = 0.3\n",
    "config.store_starting_from_ratio = 1\n",
    "config.required_improvement = 0.8\n",
    "config.model_dir = '../models/qm9/GAT2/'\n",
    "config.num_workers = 4\n",
    "config.dfs_codes = '../datasets/qm9_geometric_work/min_dfs_codes.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b969844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(config.dfs_codes, 'r') as f:\n",
    "    dfs_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "738b67cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 9, 1, 13, 6, 7, 8, 10, 11, 12, 2, 3, 0, 14]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_codes['gdb_55']['dfs_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a52085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_2_one_hot(data, dfs_codes = dfs_codes):\n",
    "    features = data.x\n",
    "    # make atomic number a one hot\n",
    "    atomic_number = nn.functional.one_hot(features[:, 5].long(), 100)\n",
    "    # make num_h a one hot\n",
    "    num_h = nn.functional.one_hot(features[:, -1].long(), 9)\n",
    "    dfs_indices = nn.functional.one_hot(torch.LongTensor(dfs_codes[data.name]['dfs_indices']), 29)\n",
    "    data.x = torch.cat((features[:, :5], features[:, 6:-1], atomic_number, num_h, dfs_indices), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2606045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = config.target_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8904130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QM9('../datasets/qm9_geometric_work/', transform=int_2_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd471b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:config.n_train]\n",
    "valid_dataset = dataset[config.n_train:config.n_train+config.n_valid]\n",
    "test_dataset = dataset[config.n_train+config.n_valid:]\n",
    "config.n_test = len(test_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, pin_memory=True, num_workers=config.num_workers)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a94cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(config.model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6baac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset.indices(), config.model_dir+'dataset_indices.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be0c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12914486",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d7c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786e4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://schnetpack.readthedocs.io/en/stable/tutorials/tutorial_02_qm9.html\n",
    "# and https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/schnet.html#SchNet\n",
    "for data in train_loader:\n",
    "    data = data.to(device)\n",
    "    atomU0s = torch.tensor(qm9.atomrefs[target_idx], device=device)[torch.argmax(data.x[:, :5], axis=1)]\n",
    "    target_modular = scatter(atomU0s, data.batch, dim=-1, reduce='sum')\n",
    "    target_vec += [(data.y[:, target_idx] - target_modular).detach().cpu().numpy()]\n",
    "target_vec = np.concatenate(target_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c96e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = np.mean(target_vec)\n",
    "target_std = np.std(target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d61374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionReadout(nn.Module):\n",
    "    def __init__(self, dtarget, dmodel=512, dim_feedforward=2048, nlayers=6, nhead=8):\n",
    "        self.dtarget = dtarget\n",
    "        self.dmodel = dmodel\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.nhead = nheada\n",
    "        self.nlayers = nlayers\n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, self.d_model), requires_grad=True)\n",
    "        self.enc = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.d_model, \n",
    "                                                                    nhead=self.nhead, \n",
    "                                                                    dim_feedforward=self.dim_feedforward), \n",
    "                                         self.nlayers)\n",
    "        self.fc_out = nn.Linear(self.d_model, self.dtarget)\n",
    "        \n",
    "    def forward(sequence):\n",
    "        # sequence must be of shape: [n_seq, n_batch, dmodel]\n",
    "        augmented_sequence = torch.cat((self.cls_token.expand(-1, sequence.shape[1], -1), sequence), dim=0)\n",
    "        transformer_out = self.enc(augmented_sequence)\n",
    "        out = self.fc_out(transformer_out[0]) \n",
    "        return out\n",
    "        \n",
    "\n",
    "class GATNN(nn.Module):\n",
    "    def __init__(self, vert_dim, hidden_dim=config.hidden_dim, nhead=config.nhead, nlayers=config.nlayers,\n",
    "                 #ro_dmodel=128, ro_dim_feedforward=512, ro_nlayers=6, ro_nhead=8, \n",
    "                 mean=None, std=None, atomref=None,\n",
    "                 max_vertices=29, max_edges=28):\n",
    "        \"\"\"\n",
    "        transfomer model is some type of transformer that \n",
    "        \"\"\"\n",
    "        super(GATNN, self).__init__()\n",
    "        gat_layers = [(tgnn.GATv2Conv(hidden_dim, hidden_dim, heads=nhead), 'x, edge_index -> x')]\n",
    "        for _ in range(nlayers-1):\n",
    "            gat_layers += [(tgnn.GATv2Conv(hidden_dim, hidden_dim, heads=nhead), 'x, edge_index -> x')]\n",
    "        \n",
    "        self.gat_layers = tgnn.Sequential('x, edge_index', gat_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        self.fc_out2 = nn.Linear(hidden_dim, 1)\n",
    "        self.readout = tgnn.GlobalAttention(self.fc_out, self.fc_out2)\n",
    "        \n",
    "        self.x_emb = nn.Linear(vert_dim, hidden_dim)\n",
    "        self.pos_emb = nn.Linear(3, hidden_dim)\n",
    "        \n",
    "        #TODO: maybe:\n",
    "        #self.readout = SelfAttentionReadout(ro_dmodel, ro_dim_feedforward, ro_nlayers, ro_nhead)\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.register_buffer('initial_atomref', atomref)\n",
    "        self.atomref = None\n",
    "        if atomref is not None:\n",
    "            self.atomref = nn.Embedding(100, 1)\n",
    "            self.atomref.weight.data.copy_(atomref)\n",
    "    \n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, z, edge_index, batch = data.x, data.z, data.edge_index, data.batch\n",
    "        h = self.x_emb(x) + self.pos_emb(data.pos)\n",
    "        h = self.gat_layers(h, edge_index) \n",
    "        out = self.readout(h, batch)\n",
    "        \n",
    "        \n",
    "        #batch = batch.permute(1, 0, 2) # seq_dim x batch_dim x n_model\n",
    "        #transformer_out = self.enc(batch)\n",
    "        #out = self.fc_out(transformer_out[0]) \n",
    "        \n",
    "        # tricks from Schnet\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            out = out * self.std + self.mean\n",
    "                \n",
    "        if self.atomref is not None:\n",
    "            out = out + tgnn.global_add_pool(self.atomref(z), batch)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a80a7aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "model = GATNN(next(iter(train_loader)).x.shape[1], atomref=dataset.atomref(target_idx), mean=target_mean, std=target_std)\n",
    "loss = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', verbose=True, patience=config.patience, factor=config.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28549be9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66509662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "Epoch 1: MAE/CA 57.410989: : 860it [01:07, 12.72it/s] \n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "Epoch 2: MAE/CA 32.545458: : 25it [00:01, 14.35it/s]"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "min_mae = config.store_starting_from_ratio\n",
    "try:\n",
    "    # For each epoch\n",
    "    for epoch in range(config.n_epochs):\n",
    "        # For each batch in the dataloader\n",
    "        pbar = tqdm.tqdm(enumerate(train_loader, 0))\n",
    "        epoch_loss = 0\n",
    "        for i, data in pbar:\n",
    "            model.zero_grad()\n",
    "            data.to(device)\n",
    "            target = data.y[:, target_idx]\n",
    "            prediction = model(data)\n",
    "            output = loss(prediction.view(-1), target)\n",
    "            mae = (prediction.view(-1) - target).abs().mean()\n",
    "            epoch_loss = (epoch_loss*i + mae.item())/(i+1)\n",
    "            \n",
    "            pbar.set_description('Epoch %d: MAE/CA %2.6f'%(epoch+1, epoch_loss/chemical_accuracy[target_idx]))\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({'MSE': output.item()})\n",
    "        curr_lr = list(optimizer.param_groups)[0]['lr']\n",
    "        wandb.log({'MAE':epoch_loss, \n",
    "                   'MAE/CA':epoch_loss/chemical_accuracy[target_idx],\n",
    "                   'learning rate':curr_lr})\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        loss_hist += [epoch_loss] \n",
    "\n",
    "        if epoch_loss/chemical_accuracy[target_idx] < min_mae*config.required_improvement:\n",
    "            min_mae = epoch_loss/chemical_accuracy[target_idx]\n",
    "            torch.save(model.state_dict(), config.model_dir+'gat_epoch%d.pt'%(epoch+1))\n",
    "        if curr_lr < config.minimal_lr:\n",
    "            break\n",
    "        if epoch_loss/chemical_accuracy[target_idx] < config.target_ratio:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt caught')\n",
    "    torch.save(model.state_dict(), config.model_dir+'gat_epoch%d.pt'%(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm.tqdm(enumerate(test_loader, 0))\n",
    "epoch_loss = 0\n",
    "maes = []\n",
    "for i, data in pbar:\n",
    "    data.to(device)\n",
    "    prediction = prediction = model(data.x, data.z, data.edge_index, data.batch)\n",
    "    mae = (prediction.view(-1) - data.y[:, target_idx]).abs()\n",
    "    maes += [mae.detach().cpu()]\n",
    "maes = torch.cat(maes, dim=0)\n",
    "mae = maes.mean().item()\n",
    "print(mae, mae/chemical_accuracy[target_idx])\n",
    "wandb.log({'TEST MAE':mae, 'TEST MAE/CA':mae/chemical_accuracy[target_idx]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
